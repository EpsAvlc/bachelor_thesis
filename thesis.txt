摘要

摘 要

近年来，无人车自动驾驶与导航技术的蓬勃发展催生了无人车产业的蓬勃兴
起。无人车领域中，三维障碍物检测一直是一个热门的话题。良好的障碍物检测
结果有利于无人车的路径规划以及自助避障的实现。目前针对三维障碍物，应用
最广泛的传感器为激光雷达（LiDAR)。而激光雷达的点云数据具有稀疏性与不一
致性，这给三维障碍物检测带来了挑战。
因此，本文提出了一种新颖的三维感知机构，能够通过给激光雷达提供航向
角方向的往复运动来增加激光雷达点云的分辨率，这有助于提升基于激光雷达目
标检测的准确率；同时本文还利用帧内多次线性插值的方式矫正了由于激光雷达
作往复运动而产生的运动畸变，大大提升了点云多帧融合的效果。
此外，本文还通过标定激光雷达与相机的外参，融合了激光雷达与相机的数
据，对三维障碍物进行了检测与定位；同时利用激光雷达的点云信息对相机的目
标检测结果做了优化，提高了基于图像的目标检测的查准率。
关键词：无人车，障碍物检测，激光雷达，点云，多传感器融合

I

ABSTRACT

ABSTRACT

The activity in the area of unmaned ground vehicles navigation in recent years has
initiated a eries of reactions that stirred the automobile industry. In the area of unmaned
ground vehicles, 3d obstacle detection has always been a hot topic. A good obstacle result
may benefit UGV’s route plan and obstacle avoidance.For detecting 3d obstacles, LiDAR
is widely used.However, 3d points Lidar’s point cloud are unorderde and sparse, which
challenging the task of 3d obstacle detetction.
A novel mechanism is proposed in this paper to handle the sparse point cloud by
reciprocating the Lidar sensor in the heading direction.In addition, this paper proposes a
method which recitifys the motion distortion of LiDAR by multiple lineary interpolation
in a single point cloud, thus imporves the performance of multiple frame fusion.
Futhermore, camera image and lidar data are fused after calibing the lidar and cameras’s extrins paramters.This paper also uses point cloud to optimize the performance of
objection detection accuracy based on image.
Keywords: unmaned ground vehicles, obstacle detection, LiDAR, point cloud, multiple
sensors fusion

III

目录

目 录

第1章 绪论 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1

1.1 研究工作的背景与意义 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1

1.2 无人车三维障碍物检测的国内外研究历史与现状 . . . . . . . . . . . . . . . . . . . .

2

1.2.1 基于相机的检测方法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2

1.2.2 基于激光雷达的检测方法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2

1.2.3 多传感器融合的检测方法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3

1.3 本文的主要贡献与创新 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3

1.4 本论文的结构安排 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3

第2章 三维感知传感器机构设计 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5

2.1 三维感知传感器机构的机械结构设计 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5

2.1.1 机械结构运动原理 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6

2.1.2 曲柄连杆机构的设计 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7

2.2 三维感知传感器机构的电路设计 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9

2.2.1 驱动器 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9

2.2.2 执行机构 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10

2.2.3 传感器 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10

2.2.3.1 角度传感器 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10

2.2.3.2 激光雷达 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10

2.2.4 主控板 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10

2.2.5 电路拓扑 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

11

2.3 三维感知传感器机构的软件设计与运动控制 . . . . . . . . . . . . . . . . . . . . . . . .

11

2.4 本章小结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

13

第3章 点云的多帧融合与激光雷达和相机标定 . . . . . . . . . . . . . . . . . . . . . . . . . . . .

14

3.1 激光雷达点云的多帧融合 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

14

3.1.1 一种朴素的多帧融合策略 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

14

3.1.2 点云的运动畸变的形成与矫正 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

15

3.1.2.1 点云的运动畸变 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

15

V

目录

3.1.2.2 运动畸变的矫正 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

16

3.1.3 矫正运动畸变后的多帧融合策略 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

18

3.2 激光雷达与相机的标定 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

19

3.2.1 标定板 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

19

3.2.2 相机坐标系中的三维特征点提取 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

20

3.2.3 LiDAR坐标系中的三维特征点提取 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

21

3.2.4 坐标系欧式变换求解 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

21

3.2.5 多帧坐标系变换结合 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

24

3.3 本章小结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

24

第4章 基于视觉与三维点云融合的三维障碍物检测方法 . . . . . . . . . . . . . . . . . . .

26

4.1 YOLO-一种实时目标检测网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

26

4.2 基于YOLO的视觉、三维点云结合的三维障碍物检测 . . . . . . . . . . . . . . . . .

28

4.2.1 相机成像原理 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

28

4.2.2 激光雷达点云的投影 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

30

4.2.3 点云前景与背景的分割 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

31

4.2.4 目标三维坐标的计算与检测结果的优化 . . . . . . . . . . . . . . . . . . . . . . . . .

33

4.3 本章小结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

34

第5章 实验验证与结果分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

36

5.1 Gazebo下的三维感知机构仿真 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

36

5.2 三维感知机构结合里程计信息构建三维地图 . . . . . . . . . . . . . . . . . . . . . . . .

37

5.3 基于点云投影到深度图像上的物体分割 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

39

5.3.1 地面点的去除 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

39

5.3.2 点云到深度图像的投影 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

41

5.3.3 基于深度图像的物体分割 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

44

5.4 本章小结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

45

第6章 全文总结与后续工作展望 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

46

6.1 全文总结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

46

6.2 后续工作展望 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

47

参考文献 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

48

致 谢 .................................................................

50

外文资料原文 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

51

外文资料译文 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

53

VI

第1章 绪论

第1章 绪论
1.1 研究工作的背景与意义
近几年来，自动驾驶技术取得了长足的进步，而其中关键的技术就是多传感
器的环境感知与融合。环境感知的一个重要环节便是障碍物检测。目前，虽然基
于图像的障碍物检测已经取得了卓有成效的进步，然而相较于三维障碍物检测，
二维障碍物检测有以下缺陷：
1.基于单目相机的障碍物检测没有尺度信息，无法恢复出目标的三维坐标。
2.基于双目相机的障碍物检测，当基线较短时，测量距离较长（5m以上）的
物体时计算出来的距离信息很不准确，而当基线较长时，近处物体的检测
又容易出现在两个相机的视野盲区之中，从而导致无法三角化而得出距离
信息。
基于上述原因，越来越多的目光聚焦在了基于激光雷达（LiDAR）的三维障
碍物检测。LiDAR是Light Detection And Ranging的缩写，中文译作“激光探测与
测量”，一般指多线数的三维激光雷达传感器。相较于相机图像，激光雷达的点
云拥有以下几点优势：
1.测量范围广。目前的激光雷达的测量有效距离基本都在0.5-100米左右，远
高于双目相机三角测距的适用范围。
2.测量精度高。激光雷达的测距误差可达厘米级，同样优于双目相机的测距
结果。
目前最常见的旋转式激光雷达，其本质是多个激光束旋转后对每个时刻的测
距结果进行保留与注册，最后再以点云的形式发布出去。决定激光雷达的竖直方
向分辨率的参数为其激光束的数量，一般称之为激光雷达的线数。目前常用的激
光雷达线数有16线、32线、64线等，其中由于低线数的激光雷达生成的点云在测
量远距离物体时竖直方向分辨率较低。因此，低线数激光雷达点云的稀疏性较大
地制约了三维障碍物检测任务的准确率。
通常在自动驾驶的无人车系统中会在车的四周装上多个16线的激光雷达进行
点云融合，或者直接采用线数更高的激光雷达来做障碍物检测的任务。然而目前

1

电子科技大学学士学位论文

三维激光雷达造价不菲，无人车系统中光是64线激光雷达的成本就奖金十万美金，
如此高昂的成本在一定程度上限制了无人驾驶汽车的普及与推广。
鉴于上述存在在问题，本文希望能够提出一种基于多帧融合的低线数激光雷
达感知机构，使其能够通过增加一个在垂直方向的往复运动，并将该机构上激光
雷达的多帧点云融合发布来提高三维点云的稠密性，借而解决低线数激光雷达在
障碍物检测问题上由于点云的稀疏性而造成的困难。并且，本文还希望通过融合
上述机构发布的点云信息以及相机的图像信息，发挥各个传感器的优势从而提高
三维障碍物检测任务的准确率与实现三维障碍物的多类别检测。

1.2 无人车三维障碍物检测的国内外研究历史与现状
根据本文的主要研究方向，下面将对面向无人车的三维障碍物检测方面的研
究展开调研。
目前，面向无人车的三维障碍物检测主要可以分为基于相机的检测方法、基
于雷达的检测方法和多传感器融合的检测方法这三种类型。

1.2.1 基于相机的检测方法
基于相机的检测方法目前主要是以基于深度学习的物体检测为主。随着 ImageNet数据集[1] 的建立以及大规模目标检测竞赛的进行，深层卷积网络被成功运用
在了图像识别与物体检测领域，而通过深度学习来实现物体的识别与分类，正是
目前无人车障碍物检测的主流方法。然而单靠相机较难获得三维位置信息，因而
其往往作为其他传感器定位后的辅助分类设备。

1.2.2 基于激光雷达的检测方法
激光雷达通过发射激光并测量其返回时间，得到距离信息，测量距离远、速
度快、误差小并且分辨率高。基于激光雷达的障碍物检测一般可分为基于模型与
无模型两种类别。基于模型的检测方法[2] 通过先验知识构建模型来同时进行点云
的检测与分类，但是计算量巨大并且很难做到实时。无模型的方法则首先采用模
型估计地面[3] 来去除地面点云，为了减少计算量，将剩余点云投影到地面[4] 或者
一个虚拟的平面上生成图像（称之为深度图（Range image）） [5] ，之后再从图像
上进行障碍物的检测与分割。

2

第1章 绪论

另外亦有国外研究提出3D检测网络将特征提取和边界框预测统一到单个阶段
的端到端可训练深度网络中，通过深度学习同时解决点云的检测与分类问题[6] 。
其深度学习网络输入为点云，通过对点云预处理和卷积后得出障碍物的包围盒以
及类别。

1.2.3 多传感器融合的检测方法
在实际应用中，无人驾驶汽车障碍物检测的主流方法主要是利用雷达与图像
融合[7] 的方法。单纯利用相机缺点是获取准确的三维信息比较困难，并且受环境
光线的影响比较大；而对于激光雷达，由于点云的稀疏不一致性，使得单纯依靠
点云进行目标检测的准确率较低。而将激光雷达与相机结合，则可以取长补短，
实现对周边物体的检测分类与定位。

1.3 本文的主要贡献与创新
针对低线数激光雷达在三维障碍物检测问题中因为点云的稀疏性与不一致性
给三维目标检测带来的困难，本文提出了一种新颖的三维感知机构，通过给激光
雷达提供偏航角方向上的往复运动，并且融合多帧点云来增加激光雷达点云在竖
直方向上的分辨率，同时通过帧内多次线性插值来矫正因机构运动而带来的点云
的运动畸变。
同时，本文还通过对相机与激光雷达的标定，融合了相机图像与激光雷达的
传感器数据信息，借由相机的目标检测算法来对物体进行检测，通过激光雷达的
点云信息来对检测出的三维障碍物进行定位，并且利用点云信息优化了视觉的目
标检测的结果。

1.4 本论文的结构安排
本文的结构安排如下：
1.第一章为绪论，介绍面向无人车的三维障碍物检测的研究背景与本文的主
要研究内容。
2.第二章为三维感知机构的设计，介绍了本文提出的一种三维感知机构的机
械、电路、软件方面的设计。

3

电子科技大学学士学位论文

3.第三章介绍了点云的畸变矫正以及相机与激光雷达的标定，为后文的相机
与激光雷达的数据融合提供外参。
4.第四章介绍了相机与激光雷达的融合，利用相机图像对障碍物进行检测与
分类，同时利用激光雷达的点云信息来对障碍物进行定位。
5.第五章根据前文提到的三维感知机构，设计了仿真实验与基于点云投影到
深度图像的物体分割。

4

第2章 三维感知传感器机构设计

第2章 三维感知传感器机构设计

为了能够解决低线数激光雷达在障碍物检测问题上由于点云的稀疏性而造成
的困难，本章提出了一种三维感知传感器机构，通过增加三维激光雷达在垂直方
向的往复旋转，同时融合多帧激光点云，来增加激光雷达在铅垂方向上的分辨率，
实现类似于高线数激光雷达的稠密点云。

2.1 三维感知传感器机构的机械结构设计

(a)

(b)

图 2-1 机构总图(a)solidworks渲染图;(b)实物图

5

电子科技大学学士学位论文

2.1.1 机械结构运动原理
本章所述的机构结构如图2-1(b)所示，其中3508电机提供驱动转矩，曲柄连杆
装置将电机的旋转运动转化为激光雷达底座在航向角(yaw)方向上的往复运动，同
时绝对值磁编码器记录激光雷达在航向角上的角度变化，以供多传感器融合时使
用。

X

Y

Si
deVi
ew

TopVi
ew

图 2-2 激光雷达坐标系

激光雷达自身的坐标系如图2-2所示，为右手系，其坐标原点在激光雷达的体
心。由于激光雷达的传感器性质，导致其在Z轴方向上的点云十分稀疏，因此，本
文希望通过机构让激光雷达绕Y轴（也就是上文所述的航向角方向）做往复旋转
运动，并通过注册多帧激光雷达的点云来实现激光雷达点云在Y轴方向上的稠密
化。
在著名的激光SLAM算法LOAM[8] 中，由于当时的条件限制，其文章作者没
有三维激光雷达来进行SLAM，而是用一个舵机给一个二维激光雷达进行竖直方
向的往复旋转来增加线数，如图2-3所示。
该机构最大的优点就是结构简单，这也是本文初次采用的机构设计。然而该
机构有以下几个缺点：
1.在往复运动中，当运动方向发生改变时，由于舵机控制精度的问题，很难
做到平滑换向，并且经常伴随有较大的震动，给之后的传感器融合算法带
来了困难。

6

第2章 三维感知传感器机构设计

图 2-3 LOAM中的机构设计

2.LOAM中采用的是二位激光雷达，重量较轻，而本文需要带动三维激光雷
达进行往复运动，重量较重（近1kg），长时间使用舵机带动会使舵机产生
较为明显的回程间隙，影响角度的测量与后续的传感器融合的效果。
因为上述原因，我们没有采用这种结构设计，而是采用了之前提到的曲柄连
杆机构来针对三维激光雷达进行往复运动，相较于上述机构，曲柄连杆结构有以
下几个优点：
1.换向平滑。执行电机只需要一直向同一方向旋转，曲柄连杆机构就能够自
动换向，并且输出的角度曲线近似正弦曲线。
2.对执行机构负担小。仅需要较小并且较为恒定的转矩就能够驱动较大的负
载做往复运动。
3.对执行机构的控制要求低。在该机构中，无刷电机只需要输出恒定的转矩
就能够完成三维激光雷达在偏航角方向上的往复运动，并且经过验证，其
角度输出近似正弦曲线，而若采用上述的舵机机构，要想得到相近的角度
曲线，则对舵机的软件控制提出了较高的要求。
综上，本文选择曲柄连杆机构作为该机构的驱动机构。

2.1.2 曲柄连杆机构的设计
该三维感知机构的一个难点在于如何设计与电机相连的曲柄连杆机构。这里
参照《机械设计基础》 [9] 一书中的相应章节对曲柄四连杆机构的连杆长度进行求
解。
如图2-4所示，假设已知该铰链四杆机构两连架杆𝐴𝐵和𝐶𝐷所形成的角度𝜓1 和𝜑1 在
三个不同位置下的角度，要求连杆a、b、c、d的尺寸。则根据向量向x、y轴投影，
有

7

电子科技大学学士学位论文

图 2-4 四杆机构的数学模型

𝑎 cos 𝜑 + 𝑏 cos 𝛿 = 𝑑 + cos 𝜑
𝑎 sin 𝜑 + 𝑏 sin 𝛿 = 𝑐 sin 𝜑
将上两式先进行移项，然后作平方和相加，从中消去𝛿后整理有

𝑏2 = 𝑎2 + 𝑐2 + 𝑑2 + 2𝑐𝑑 cos 𝜓 − 2𝑎𝑑 cos 𝜑 − 2𝑎𝑐 cos(𝜑 − 𝜓)
我们设
⎧
⎪
⎪
𝑅1 = (𝑎2 + 𝑑2 + 𝑐2 − 𝑏2 )
⎪
⎪
⎨
𝑅2 = 𝑑/𝑐
⎪
⎪
⎪
⎪
⎩𝑅3 = 𝑑/𝑎
代入，则上一个式子可以化简为
𝑅1 − 𝑅2 cos 𝜑 + 𝑅3 cos 𝜓 = 𝑐𝑜𝑠(𝜑 − 𝜓)
这个式子即为铰链四连杆机构的角位置方程， 该方程有三个待定参数𝑅1 、 𝑅2 、
𝑅3 。故应有三组对应的𝜓1 和𝜑1 角才能得出这个方程的解。将三组𝜓1 和𝜑1 角代入
求解该方程后，可以得到四个构件之间的长度关系为
⎧
⎪
⎪
𝑎 = 𝑑/𝑅3
⎪
⎪
⎨
𝑐 = 𝑑/𝑅2
⎪
⎪
√︀
⎪
⎪
⎩𝑏 = 𝑎2 + 𝑐2 + 𝑑2 − 2𝑎𝑐𝑅1

8

第2章 三维感知传感器机构设计

则根据机构的具体设置情况，知道𝑎, 𝑏, 𝑐, 𝑑中的任何一条边的长度后，便可知
剩下四条边的长度。
在实际设计中，我们已知𝜓1 和𝜑1 的三组对应角度为
⎧
⎪
⎪
𝜓1 = 30 ∘ 𝜑1 = 36.3∘
⎪
⎪
⎨
𝜓1 = 60 ∘ 𝜑1 = 43.87∘
⎪
⎪
⎪
⎪
⎩𝜓1 = 120 ∘ 𝜑1 = 35.75∘
并且根据我们的机构设置，构件𝑑的长度为105.72mm。将这些已知量代入公式中
可得
⎧
⎪
⎪
𝑎 = 31.6𝑚𝑚
⎪
⎪
⎨
𝑏 = 49.18𝑚𝑚
⎪
⎪
⎪
⎪
⎩𝑐 = 108.37𝑚𝑚
由此，便得到了曲柄机构的连杆构件设计参数。

2.2 三维感知传感器机构的电路设计

(a)

(b)

(c)

(d)

图 2-5 驱动器与传感器(a)C620 电调;(b)M3508无刷电机;(c)磁编码器;(d)RM A型开发板

2.2.1 驱动器
该三维感知机构采用的驱动器为DJI C620电调，如图2-5(a)所示。该电调支
9

电子科技大学学士学位论文

持50-500Hz的PWM（脉宽调制）信号控制以及CAN总线指令控制，最高支持20A的
持续电流，支持对CAN总线上的电调快速设置ID，支持通过CAN总线获取电机温
度、转子位置和转子速度等信息，切换电机时可无需进行位置传感器的参数校
准。

2.2.2 执行机构
该三维感知机构采用的执行机构为DJI M3508无刷电机，如图2-5(b)所示。该
电机可搭配上文所述C620电调实现正弦驱动，相比传统方波驱动具有更高的效
率、机动性和稳定性。其最高可持续输出力矩为2.8Nm，满足驱动曲柄四连杆机
构的需求。

2.2.3 传感器
2.2.3.1 角度传感器

该三维感知机构采用的角度传感器为傲蓝13线磁编码器，如图2-5(c)所示。该
编码器采用RS485方式通信，其单圈分辨率为8192cpr, 精度为±0.1度。
该编码器为绝对值式编码器，其相对于增量式编码器不同点在于，增量式编
码器以上电时的位置为零点，每次使用都要机械对位；而绝对值式编码器能够记
录机构的唯一位置，即单圈内编码器的每一个示值，都唯一对应了空间中机构的
位置与角度。考虑到我们曲柄连杆机构的特性，显然绝对值式编码器更加符合我
们的要求。
2.2.3.2 激光雷达

该三维感知机构采用的激光雷达为速腾聚创的RS-LiDAR-16，如图2-6所示。
该激光雷达为16线激光雷达，其测距范围为50cm-150m，精度误差为±2𝑐𝑚。垂
直视场角为30度， 其角分辨率为2度； 水平视场角为360度， 其角分辨率为0.090.36度（对应的点云频率为5Hz-20Hz）。

2.2.4 主控板
该三维感知机构采用的主控板为DJI Robomaster A型开发板， 如图2-5(d)所
示。该开发板具备类型丰富的接口，包括12V、5V、3.3V电源接口、CAN接口、
UART接口、 可变电压PWM接口、 SWD接口等。 同时该开发板拥有电源输入的
防反接、过压保护、缓启动、12V电源输出过流保护、PWM端口的ESD等多重保
护。
10

第2章 三维感知传感器机构设计

图 2-6 速腾16线激光雷达

2.2.5 电路拓扑
该三维感知机构的电路拓扑如图2-7所示。激光雷达通过千兆网接口将点云传
输到mini PC上，磁编码器通过485转USB与mini PC通信，同时主控通过PWM控制
电调输出，调节M3508电机的转速，M3508电机提供曲柄四连杆机构的驱动力矩，
而磁编码器又将曲柄机构作用在激光雷达底座上的旋转通过485通信输出到mini
PC。

2.3 三维感知传感器机构的软件设计与运动控制
根据上文所述的机械设计以及电路设计，该三维感知机构的软件设计主要实
现了以下几个任务：
1.实现了各个传感器、 主控到Mini　PC的通信， 同时将数据以ROS （Robot
Operating System）话题的方式发布出去，以供第二章节提到的多帧融合算
法使用。
2.实现了电机的多档调速功能。为了应对不同的场景，在主控中实现了多档
调速功能，以调节曲柄机构的往复运动频率。
3.实时检测电调的温度信息，提供了基于温度检测的堵转保护（温度过高自
动切断控制）。
此外，本文还记录了在电机输出恒定转速情况下的曲柄连杆机构的输出的角
度信息，如图2-8所示。该图纵坐标为角度制的输出角度。从图中可以看出，本章
11

电子科技大学学士学位论文

图 2-7 电路拓扑

12

第2章 三维感知传感器机构设计

所设计的三维感知机构其输出角度近似正弦曲线，并且没有较大的换向震动，相
比起上文所提到的舵机的结构拥有稳定可靠的优势。

图 2-8 曲柄连杆机构输出角度

2.4 本章小结
本章提出了一种新的三维感知机构，并从该机构的机械设计、电路设计以及
软件设计和运动控制三个方面介绍了该机构。在机械方面，本文提出使用无刷电
机+曲柄摇杆机构来代替简单的舵机给三维激光雷达提供一个竖直航向角方向上
的往复运动，这种结构的优势在于机构换向流畅、控制简单以及对机构负载小，
能够为本文后续章节提到的激光雷达的多帧融合提供结构上的稳定与可靠性。在
电路方面，本章利用绝对值式磁编码器对曲柄机构运动的角度进行了记录与输
出，相较于增量式编码器，磁编码器不需要保证每次上电时机构都在同一个位置，
为机械结构的设计提供了便利。在软件方面，本章实现了各个传感器与主控以
及mini PC的通信，主控对无刷电机的多档控制以及对电机的堵转保护，并且绘制
了输出角度，验证了机构的可行性。本文的后续章节将利用该机构输出的点云信
息以及角度信息来进行点云的多帧融合稠密化，并且在融合的点云上进行三维障
碍物的检测与分类。

13

电子科技大学学士学位论文

第3章 点云的多帧融合与激光雷达和相机标定

根据本文第二章所提到的机构，能够将三维激光雷达在其yaw角方向上提供
一个有规律的正弦往复运动。本文提出该机构的主要目的为将激光雷达在时间轴
上的多帧点云进行融合，进而增加激光雷达在竖直方向的分辨率，达到近似于给
激光雷达增加线数的效果。本章将对上述机构得到多帧激光雷达点云进行融合，
并且在目前机构的基础上进行激光雷达与相机的标定，从而使得融合后的点云能
够应用于第四章提到的基于视觉与激光融合的三维障碍物检测方法。

3.1 激光雷达点云的多帧融合
3.1.1 一种朴素的多帧融合策略
点云的注册（registration）是指将有重合部分的点云进行对齐的一项技术。其
关键核心为求出给定点云的坐标系相对于目标点云所在坐标系的旋转与平移，借
以将给定点云转换到目标点云坐标系中，丰富目标点云的信息，以给之后的点云
分割与分类任务提供便利。

图 3-1 三维感知机构的旋转

14

第3章 点云的多帧融合与激光雷达和相机标定

如图3-1所示，本文认为当激光雷达航向角与地面平行时，其激光雷达坐标
系为参照系𝑓 ，第二章所述的三维感知机构目的就在于在激光雷达做往复运动时，
将激光雷达每帧点云在参照系𝑓 上进行注册，最后在曲柄机构的一个运动周期后
将点云融合输出。当机构运动时，激光雷达自身坐标相对于𝑓 发生了旋转，因而
在激光雷达坐标系中的点云相对于𝑓 有一个𝛼角度的旋转量。因而要将此时的点云
注册到𝑓 坐标系中时，要抵消因机构旋转而造成的坐标系的旋转。
最为直观的策略就是，读取绝对值编码器返回的角度𝛼，将激光雷达每帧点
云沿着航向角方向旋转−𝛼的角度，然后注册多帧的激光雷达点云并发布。
在实际的实现过程中，编码器返回角度的频率约为30Hz，而点云发布的频率
约为10Hz，在将点云旋转−𝛼角度时，对𝛼角进行了线性插值以便获得更加精确的
结果。同时，根据计算曲柄机构的角度是增加还是减少，来判断曲柄机构的运动
方向，并且将曲柄机构运动角度为一个正弦周期内的点云融合为一帧新的点云输
出。
然而这种朴素的融合策略在实际中效果不好，体现在融合后的点云所显示的
物体轮廓失真严重，如图3-4(a)所示，原因是没有考虑激光雷达的运动对激光雷达
点云生成的影响。

3.1.2 点云的运动畸变的形成与矫正
在激光雷达点云的多帧融合中，如果只是进行简单的历史点云叠加（如上文
所示），那么融合后的点云相较于真实情况会有很严重的失真，其原因就在于第
二章所提到的三维感知机构在给激光雷达在偏航角方向上的往复运动时，点云会
产生不可忽视的运动畸变。本章节首先介绍什么是激光雷达点云的运动畸变，然
后提出一种通过插值的方式矫正激光雷达的运动畸变。
3.1.2.1 点云的运动畸变

激光雷达的点云的形成本质上是由激光雷达内部的多个激光测距器将一个旋
转周期内的各个测量值记录下来并同时发布后得到的。因此点云中的每个点并不
是在同一时刻被测量出来的。如果激光雷达在测量的过程中也在运动，那么激光
雷达的点云可能会发生畸变[10] 。
下面以二维激光雷达为例，介绍激光雷达点云运动畸变的形成。
图3-2(a)中的黑色的线条表示二维激光雷达处在的真实环境的轮廓图，箭头表
示二维激光雷达的运动方向。图3-2(b)中的蓝色的线条表示二维激光雷达的原始
15

电子科技大学学士学位论文

(a)

(b)

图 3-2 激光雷达运动畸变(a)ground truth;(b)采集得到的数据

数据。注意到其已经发生了畸变，因为二维激光雷达内的激光测距器通过逆时针
的方向旋转，使得右上角的数据优先得到，而左上角的数据在激光雷达向箭头方
向运动了一段距离之后才进行测量，自然导致了运动畸变的产生。
值得一提的是上图表示的二维激光雷达发生的运动畸变是当激光雷达运动方
向为水平运动方向时造成的，而当激光雷达在空间中有垂直方向的旋转运动时，
其造成的运动畸变远比水平运动严重。这是因为激光雷达旋转一周的时间普遍
在0.1秒左右，其水平运动的距离往往很小可以忽略不计。而当激光雷达有竖直方
向的旋转时，即使在0.1s内只有2度的航向角的旋转（这在本文的机构中并不算很
快），在测量20m处的物体时，其运动造成的点云畸变可使得点云的同一线上的第
一个点与最后一个点的垂直相差将近70cm。
综上所述，对于第二章所述的机构，由于其施加了在激光雷达航向角方向上
的旋转，因此导致其在竖直方向上的运动畸变不可忽视，从而简单的叠加点云会
导致在做激光雷达的物体检测时的失真。
3.1.2.2 运动畸变的矫正

对于本文所提到的三维感知机构在航向角方向上的旋转所产生的运动畸变的
矫正，一个较为朴素的方法是，依次遍历激光雷达每帧点云中的每个点，计算其
产生的时间戳𝑡，对磁编码器的角度进行插值，计算出在时间戳𝑡上的角度𝛼，然后
将该点绕原点在航向角方向旋转−𝛼的角度。这个方法最为直观，然而激光雷达每
帧点云高达数十万个点，如果对每个测量得到的点进行插值，则在一秒内要进行
近百万次的插值与旋转操作，显然对于无人驾驶汽车上的移动处理器平台来说这
是不现实的。
16

第3章 点云的多帧融合与激光雷达和相机标定

因此本文提出一个假设，设点云中第一个点产生的时间戳为𝑡0 ，最后一个点
产生的时间戳为𝑡𝑘 ， 则将𝑡0 到𝑡𝑘 之间的时间均匀分为𝑛份， 每份长为Δ𝑡。 本文假
设𝑡0 至𝑡0 +Δ𝑡、𝑡0 +Δ𝑡至𝑡0 +2Δ𝑡...𝑡𝑘 −Δ𝑡至𝑡𝑘 这些时间段，每个时间段内的激光雷
达的测距点的产生的时间都是相同的，为其第一个点的产生时间。根据这个假设，
每个时间段内的所有点都只要进行相同角度变换即可进行运动畸变的矫正。遵循
该假设，则每帧点云只需要进行n次角度插值即可，极大地减小了运算量。同时
虽然该假设认为同一时间段内的点云是同一时间产生的，每个时间段内的点仍有
运动畸变的影响，然而在实验过程中发现，只要当𝑛取一个不太小的值（𝑛 ≥ 50），
则该假设的所产生的时间段内的运动畸变产生的影响很小，可忽略不计。
在实际的程序实现中，由于激光雷达每帧点云是分段传输的，如图3-3所示。
RS-LiDAR-16激光雷达采用UDP协议向PC传输点云信息，而UDP协议相较于TCP协

图 3-3 点云的传输

议，发送数据之前不需要双方建立链接，并且发送的数据没有校验，也没有丢包
的检测，因此不适合一次性发送大量数据给PC。该激光雷达将一帧点云分为84个
UDP包(UDP Packet)，每个包中的点云都是激光雷达旋转360/84 = 4.28度后得到
的16线的点云的集合。当激光雷达的驱动程序接收到84个UDP Packet之后，将这84
个Packet合并成一帧点云输出。
本文修改了RS-LiDAR-16的ROS 驱动程序，将每个UDP Packet不经过合并直
17

电子科技大学学士学位论文

接发布出去，同时在多帧融合的程序里，对每个Packet（而不是每帧）分别进行
编码器角度的插值与点云的旋转，最后再将旋转后的84个Packet进行合并发布。

3.1.3 矫正运动畸变后的多帧融合策略
如上文所言，对单帧点云进行多次插值之后的多帧融合算法流程如Algorithm
1所示；
。
在Algorithm 1中，FindeClosestAngles函数查找在所有的编码器角度中，时间
上离packet的时间戳最近的两帧编码器的角度值，随后对这两个角度进行线性插
值得到packet时间戳下的机构的角度。RotatePointCloudByYaw函数将特定的点云
绕激光雷达坐标系原点旋转指定的角度。最后将每个矫正后的packet合并到一个
新的矫正后的点云中并发布出去即得到了矫正因机构而产生的运动畸变后的点
云。
在图3-4(b)中展示了消除运动畸变后的激光雷达测得的轿车的点云图案。相
较于图3-4(a)，其点云图案没有出现明显的失真，并且通过机构进行多帧融合后的
点云，能够明显的看出轿车的轮廓细节信息，包括车的反光镜、前挡风玻璃等。
而矫正畸变前的轿车点云则很难分辨出这些细节，并且由于运动畸变的作用，其
体积明显比矫正后的点云更大一些。由此证明了本文提出的多帧融合算法拥有较
好的矫正畸变的效果。

(a)

(b)

图 3-4 多帧融合后的轿车点云(a)矫正畸变前;(b)矫正畸变后

18

第3章 点云的多帧融合与激光雷达和相机标定

Algorithm 1 Improved multiple pointcloud fusion algorithm
1: for packet ∈ point cloud do
2:

𝛼1 , 𝛼2 = FindeClosestAngles(encorderAngles, packet.timestamp) 　　

3:

1 .𝑡𝑖𝑚𝑒𝑠𝑡𝑎𝑚𝑝
𝛼 = 𝛼1 + (𝛼2 − 𝛼1 ) × 𝑝𝑎𝑐𝑘𝑒𝑡.𝑡𝑖𝑚𝑒𝑠𝑡𝑎𝑚𝑝−𝛼
𝛼2 .𝑡𝑖𝑚𝑒𝑠𝑡𝑎𝑚𝑝−𝛼1 .𝑡𝑖𝑚𝑒𝑠𝑡𝑎𝑚𝑝

4:

rectifiedPacket = RotatePointCloudByYaw(packet, −𝛼)

5:

rectifiedPointcloud += rectifiedPacket

6:

return rectifiedPointcloud

3.2 激光雷达与相机的标定
自动驾驶无人车是一个多传感器的系统，多传感器信息的融合可以使整个无
人车系统的决策更加智能。激光雷达虽然能够获取较为精确的点云信息，然而点
云信息只包含了三维距离信息。而相机可以通过图像获得大量信息诸如颜色、纹
理信息等，但是其受光照与天气条件影响严重，并且从单目图像中无法获取三维
结构信息。为了同时收集三维信息与物体的颜色与纹理信息，激光雷达与相机经
常进行传感器的数据融合，来为多传感器系统提供更稳定的数据支持。为了进行
数据的融合，首先得知道相机坐标系与激光雷达坐标系之间的旋转与平移关系，
因此，激光雷达与相机的标定就显得尤为重要了。
本章节后续将介绍一种文献[11] 提到的,利用两张贴有ArUco Marker[12] 的标定
板所提供的3d-3d特征匹配的方法，来进行相机与激光雷达的标定。

3.2.1 标定板

图 3-5 标定环境

19

电子科技大学学士学位论文

本文参照文献[11] 提到的方法，制作了两块长约40cm，宽约27cm的长方形硬
纸板材质的标定板，并且将两块ArUco Marker粘在硬纸板的固定位置上，如图35所示。虽然一块标定板已经可以得到四组3d-3d匹配点来解决标定问题，本文仍
然采用了两块标定板，目的是构造多于四对的匹配点来减小标定误差。

3.2.2 相机坐标系中的三维特征点提取
ArUco markers是一种经过特定编码的二维码图案，用以实现对二维码自身
的定位与畸变矫正。更多细节可以参考文献[12] 。该文献提出，通过特定的机器
视觉算法检测到marker的四个角点后， 可以对marker上的二维码进行解码运算，
进而求得二维码的id与四个角点的顺序。而通过输入marker的边长后，还能够通
过PnP[13] 求解出相机坐标系到marker自身坐标系的转换。

图 3-6 ArUco markers

在本文中，将ArUco marker粘在硬纸板的矩形标定板上，并且测量得出硬纸
板的四条边长以及marker在硬纸板中的位置，即可得到硬纸板的四个角点在marker
坐标系中的位置。而本文通过ROS中aruco ros以及aruco mapping[14] 两个程序包可
以检测ArUco marker的位置，进而得到相机坐标系到marker坐标系的转换，从而
得到相机坐标系下的硬纸板的四个角点的位置。相机坐标系下角点位置的计算公
式为
⎡
⎤
𝑥𝑐𝑎𝑚𝑒𝑟𝑎
⎢
⎥
⎢𝑦
⎥
⎢ 𝑐𝑎𝑚𝑒𝑟𝑎 ⎥
⎢
⎥
⎢ 𝑧𝑐𝑎𝑚𝑒𝑟𝑎 ⎥
⎣
⎦
1

⎡

⎤

𝑅11 𝑅12 𝑅13 𝑡𝑥
⎥
⎢
⎥
⎢𝑅
⎢ 21 𝑅22 𝑅23 𝑡𝑦 ⎥
=⎢
⎥
⎢𝑅31 𝑅32 𝑅33 𝑡𝑧 ⎥
⎦
⎣
0
0
0
1

⎤

⎡

𝑥𝑎𝑟𝑢𝑐𝑜
⎢
⎥
⎢𝑦
⎥
⎢ 𝑎𝑟𝑢𝑐𝑜 ⎥
⎥
⎢
⎢ 𝑧𝑎𝑟𝑢𝑐𝑜 ⎥
⎣
⎦

(3-1)

1

其中，𝑥𝑐𝑎𝑚𝑒𝑟𝑎 是相机坐标系中角点的坐标，𝑥𝑎𝑟𝑢𝑐𝑜 是ArUco marker坐标系中
的角点坐标，而上式中的[𝑅|𝑡]矩阵则是相机坐标系相对于marker坐标系的欧式变
换矩阵。
20

第3章 点云的多帧融合与激光雷达和相机标定

3.2.3 LiDAR坐标系中的三维特征点提取
本文所参照的标定方法，其在LiDAR坐标系中的三维特征点提取是通过直线
拟合的方法进行的。如图3-7所示。图中显示的点为激光雷达的点云投影到相机图
像上之后进行边缘检测后所形成的点。在得到该幅图像后，需要手动框选标定板
的每条边上的点，随后标定程序会对这些点进行直线拟合，每两条直线的交点即
为所求的LiDAR坐标系中的三维点。

图 3-7 直线拟合提取特征点

3.2.4 坐标系欧式变换求解
在得到两个坐标系下的三维特征匹配点之后，两个坐标系之间的[𝑅|𝑡]欧式变
换矩阵可以通过使用迭代最近点（Iterative Closest Point, ICP） [15] 算法求得。假
设𝑃 ，𝑄为R3 中的一组对应点，ICP算法尝试使经过欧式变换后的P点集与Q点集
的重合误差最小。其求解问题可以表述为式3-2所示。
arg min

‖(𝑅𝑃 + 𝑡) − 𝑄‖2

𝑅∈𝑆𝑂(3),𝑡∈R3

21

(3-2)

电子科技大学学士学位论文

一般来说，ICP问题认为对于点集𝑃 中的每个点，在点集𝑄中与之对应的点为其距
离最近的点，依次确认匹配关系后，该算法通过减小两个点集的欧氏距离来对齐
两个点集。
ICP的方法找到两组点的匹配关系，在没有良好的初始值的情况下，极易发
生误匹配，导致算法最后没办法收敛到一个正确的解。考虑到在本文的标定环
境下，两组点的对应关系的是已知的，则两组点之间的[𝑅|𝑡]即存在一个闭式解。
Kabsch算法[16][17] 提供了找到两组对应点间的旋转矩阵的闭式解的方法，而两组
点间的平移量可以在进行旋转对齐后求得。下面参照文献[17] ，介绍Kabsch算法的
主要步骤。
首先假设旋转已知，求两个点集𝑃 与𝑄之间的平移量，设
𝑛
∑︁
𝐹 (𝑡) =
‖(𝑅𝑃𝑖 + 𝑡) − 𝑄𝑖 ‖2

(3-3)

𝑖=1

对上式进行求导，令两边等于0，得
𝑛
∑︁
𝜕𝐹 (𝑡)
=2
((𝑅𝑃𝑖 + 𝑡) − 𝑄𝑖 ) = 0
𝜕𝑡

(3-4)

𝑖=1

因为
𝑛

𝑛

∑︁
∑︁
𝜕𝐹 (𝑡)
= 2𝑅
𝑃𝑖 + 2𝑛𝑡 − 2
𝑄𝑖
𝜕𝑡
𝑖=1

(3-5)

𝑖=1

可得
𝑛

𝑛

1 ∑︁
1 ∑︁
𝑡=
𝑄𝑖 − 𝑅
𝑃𝑖
𝑛
𝑛

(3-6)

𝑡 = 𝑄 − 𝑅𝑃

(3-7)

𝑖=1

𝑖=1

将式3-7的结果代入式3-3中，有
𝑅 = arg min

𝑛
∑︁

‖(𝑅(𝑃𝑖 − 𝑃 ) − (𝑄𝑖 − 𝑄))‖2

(3-8)

𝑅∈𝑆𝑂(3) 𝑖=1

令
𝑋 = 𝑃𝑖 − 𝑃 , 𝑋 ′ = 𝑅𝑋, 𝑌 = 𝑄𝑖 − 𝑄
则目标函数可化简为
𝑛
∑︁

2

‖𝑋𝑖′ − 𝑌𝑖 ‖ = 𝑇 𝑟((𝑋 ′ − 𝑌 )𝑇 (𝑋 ′ − 𝑌 ))

𝑖=1

22

(3-9)

第3章 点云的多帧融合与激光雷达和相机标定

使用矩阵的迹的性质，上式可化简为
𝑇

𝑇 𝑟((𝑋 ′ − 𝑌 )T (𝑋 ′ − 𝑌 )) = 𝑇 𝑟(𝑋 ′ 𝑋 ′ ) + 𝑇 𝑟(𝑌 𝑇 𝑌 ) − 2𝑇 𝑟(𝑌 𝑇 𝑋)

(3-10)

考虑到𝑅是一个旋转矩阵，旋转矩阵必定正交且行列式为1，也就是说，‖𝑋 ′ 𝑖 ‖2 =
‖𝑋𝑖 ‖2 ，则有
′

T

′

𝑇 𝑟((𝑋 − 𝑌 ) (𝑋 − 𝑌 )) =

𝑛
∑︁

(|𝑋𝑖 |2 + |𝑌 𝑖 |2 ) − 2𝑇 𝑟(𝑌 𝑇 𝑋)

(3-11)

𝑖=1

∑︀𝑛

可知

𝑖=1 (|𝑋𝑖 |

2 + |𝑌

2
𝑖 | )项与旋转矩阵𝑅无关，将其从目标函数中消去，有

𝑅 = arg min 𝑇 𝑟(𝑌 𝑇 𝑋 ′ )

(3-12)

𝑅∈𝑆𝑂(3)

将𝑋 ′ = 𝑅𝑋代入，并利用矩阵迹的性质，有
𝑇 𝑟(𝑌 𝑇 𝑋 ′ ) = 𝑇 𝑟(𝑌 𝑇 𝑅𝑋) = 𝑇 𝑟(𝑋𝑌 𝑇 𝑅)

(3-13)

对𝑋𝑌 𝑇 进行SVD分解，有𝑋𝑌 𝑇 = 𝑈 𝐷𝑉 𝑇 ，则
𝑇

𝑇

𝑇

𝑇 𝑟(𝑋𝑌 𝑅) = 𝑇 𝑟(𝑈 𝐷𝑉 𝑅) = 𝑇 𝑟(𝐷𝑉 𝑅𝑈 ) =

3
∑︁

𝑑𝑖 𝑣𝑖𝑇 𝑅𝑢𝑖

(3-14)

𝑖

令𝑀 = 𝑉 𝑇 𝑅𝑈 ，则𝑀 由正交矩阵相乘而得，故易知其亦为正交矩阵，并且𝑑𝑒𝑡(𝑀 ) =
±1。因此M的每个列向量的模均为1，列向量的每个元素均小于等于1。则有
𝑇

𝑇 𝑟(𝑋𝑌 𝑅) =

3
∑︁
𝑖

𝑑𝑖 𝑀𝑖𝑖 ≤

3
∑︁

𝑑𝑖

(3-15)

𝑖

令上式值最大，则得到𝑀𝑖𝑖 = 1，因此𝑀 = 𝐼，为单位矩阵。有
𝑀 = 𝐼 ⇒ 𝑉 𝑇 𝑅𝑈 = 𝐼 ⇒ 𝑅 = 𝑉 𝑈 𝑇

(3-16)

需要注意的是，𝑅应当是一个旋转矩阵，也就是说，𝑅 ∈ 𝑆𝑂(3)，所以应当确保
𝑑𝑒𝑡(𝑅) = +1。如果由上式得到的𝑅，其特征值为-1，则其为不满足条件的解，需
要找到使𝑇 𝑟(𝑌 𝑇 𝑋 ′ )第二大的R，即
𝑇 𝑟(𝑌 𝑇 𝑋 ′ ) = 𝑑1 𝑀11 + 𝑑2 𝑀22 + 𝑑3 𝑀33 𝑤ℎ𝑒𝑟𝑒 𝑑1 ≥ 𝑑2 ≥ 𝑑3 𝑎𝑛𝑑 |𝑀𝑖𝑖 | ≤ 1 (3-17)
在上式中，当𝑀11 = 𝑀22 = 1且𝑀33 = −1时，该式值第二大。将上述情况考虑进
去，则𝑅的闭式解为𝑅 = 𝑈 𝐶𝑉 𝑇 ，其中C为一个矫正矩阵，
⎡
⎤
1 0
0
⎢
⎥
⎥
𝐶 =⎢
0
1
0
⎣
⎦
𝑇
0 0 𝑠𝑖𝑔𝑛(𝑑𝑒𝑡(𝑈 𝑉 ))
23

电子科技大学学士学位论文

3.2.5 多帧坐标系变换结合
在实验中发现，由相机图像求得的三维特征点比较稳定，而由于激光雷达本
身存在传感器误差，其通过直线拟合求交点得到的三维角点不够稳定，在相机与
激光雷达位置均固定的情况下，每次测得的激光雷达测得的三维特征角点在会有
一个较小的位移，而这样的位移会对用Kabsch算法得到的解产生较大的误差。
为了减小因激光雷达传感器误差而造成的三维特征角点不准确的问题， 本
文在相机与激光雷达位置固定的情况下，针对多帧图像与点云进行了匹配运算，
对𝑁 次结果进行求取平均值。对平移量的平均值求取公式为
𝑁
1 ∑︁
𝑡=
𝑡𝑣𝑒𝑐𝑖
𝑁
𝑖=1

而对于旋转量的求取平均值，先将上文求得的旋转转为四元数𝑟𝑣𝑒𝑐𝑖 形式，再对四
元数求取平均值。
𝑁
1 ∑︁
𝑟=
𝑟𝑣𝑒𝑐𝑖
𝑁
𝑖=1

𝑟=

𝑟
‖𝑟‖

通过对多帧激光雷达点云与相机进行三维点匹配得到的欧式变换求取平均
值，能够有效的抑制因激光雷达的传感器误差而造成的标定误差。

3.3 本章小结
本章介绍了什么是激光雷达点云注册与运动畸变， 并且提出了插值矫正运
动畸变的方法：在对点云进行多帧融合时，利用点云传输的特性，将点云在时
间上为84个Packet单独发布， 同时线性插值得到这些Packet的时间戳上的编码器
角度值，将这些Packet点云依据编码器的角度注册到参考坐标系中，并且将所有
的Packet点云进行叠加注册后发布为新的点云。由于对每帧点云多进行了84次旋
转角度的插值，从点云图案上看，本文提到的方法极大地改善了运动畸变对点
云注册的影响，完成了矫正运动畸变的目的。同时，本章还介绍了一种利用特
制的标定板在相机与激光雷达坐标系中寻找三维特征匹配点的方法来进行相机的
标定，并详细介绍了如何利用三维匹配点来计算出两个点集所在的坐标系之间的
欧式变换（Kabsch算法）。在后续的章节，本文将利用相机与激光雷达标定出来

24

第3章 点云的多帧融合与激光雷达和相机标定

的[𝑅|𝑡]矩阵，对激光雷达与相机图像的数据进行融合处理，借以进行三维障碍物
的分割与分类。

25

电子科技大学学士学位论文

第4章 基于视觉与三维点云融合的三维障碍物检测方法

随着深度学习领域内的卷积神经网络在目标检测任务中大放异彩，基于视觉
的目标检测的深度学习算法大行其道。在著名的大规模目标检测挑战赛ILSVRC[1]
中， 基于深度学习的目标检测算法连续六年 （2012-2017） 取得了优越的表现。
2017年，ILSVRC的夺冠深度学习网络SENet在目标检测问题上的错误率仅为2.251%，
亦同时宣告了基于图像的目标检测任务基本被攻克。
然而，基于视觉的目标检测无法得到物体的三维位置信息，并且受光照影响
严重。为了解决这些问题，本章后续部分将提出一种激光雷达与相机的多模态传
感器融合技术，其能够在基于视觉的目标检测算法检测到目标时，能够根据激光
雷达的点云得到障碍物的三维位置信息，同时根据激光雷达的点云信息反馈于目
标检测任务的识别上，提升视觉在光照条件不够良好的情况下的检测准确率。

4.1 YOLO-一种实时目标检测网络
在介绍相机图像与激光雷达点云的融合之前，本文将先介绍本文使用的视觉
目标检测算法。本文使用YOLO(You Only Look Once)[18] 算法作为视觉图像上的目
标检测算法。

图 4-1 YOLO-一种实时目标检测网络

26

第4章 基于视觉与三维点云融合的三维障碍物检测方法

早期的目标检测算法通过提取图像的一些特征（例如Haar、SIFT、HOG等），
运用DPM（Deformable Parts Model）模型，并且通过滑动窗口来预测具有较高得
分的区域的策略来进行目标检测。这种传统的方法不仅相当耗时，而且检测准确
率也不是很高。
随后出现了基于object proposal策略的方法， 相比于滑动窗口这种类似于穷
举的方法，该方法大大减少了运算量，同时定位精度也得到了很大的提高。结
合13年兴起的卷积神经网络后，Object detection的性能得到了质的飞跃。
然而，诸如R-CNN、Faster R-CNN这样的网络，因为候选区域较多、网络运
算量较大，因而很难在GPU性能不够强劲的移动机器人场景中做到实时检测。而
本文采用的YOLO网络将目标检测问题转化为一个回归问题。给定图像𝐼，YOLO
网络直接在图像的多个位置上回归出识别目标的边界框（bounding box）以及其类
别。
YOLO没有选择滑动窗口或者提取proposal的策略来训练网络，而是直接将整
张图作为网络的输入，既极大地提升了运算速度，亦很好的区分了图像的前景与
背景区域，而使用proposal策略的Fast R-CNN则经常将背景区域误识别为目标区
域。

图 4-2 YOLO网络结构

图4-2为YOLO的网络结构，从图中可以看出，该目标检测网络拥有24层卷积
层，随后用两层全连接层对结果进行回归输出。
由于其简介的网络结构设计，使得其在实时性能上表现卓越。YOLO能够在
每秒推断（inference）45帧的情况下仍能够保证和Faster R-CNN同样的准确率。基
27

电子科技大学学士学位论文

于其良好的实时性与较为优秀的目标检测准确率，本文将使用YOLO作为视觉与
三维点云融合的目标检测基准方法，并利用激光雷达的点云信息为其检测结果提
供三维位置信息并提高准确率。

4.2 基于YOLO的视觉、三维点云结合的三维障碍物检测
本章所述的传感器设置如图4-3所示。其中激光雷达坐标系到相机坐标系的刚
体变换已由第二章介绍的标定方法得到。在介绍如何将激光雷达点云投影到相机
图像上之前，本文将先介绍相机的成像原理，这是点云投影的理论基础。

图 4-3 传感器设置

4.2.1 相机成像原理
所谓的相机成像，就是相机将三维空间中的坐标映射到二维图像平面的过程。
这一映射可以有许多数学模型去解释，最简单也最为常用的就是针孔成像模型，
如图4-4所示。 现在对该模型进行建模。设图中𝑂 − 𝑥 − 𝑦 − 𝑧为相机坐标系。通
常认为相机坐标系𝑍轴指向相机的前方，𝑋轴指向右方，𝑌 轴指向下方。设真实空
间中有一三维点𝑃 ，经过相机的小孔𝑂成像后，成像点落在相机的成像平面𝑃 ′ 处，
设𝑃 点的坐标为[𝑋, 𝑌, 𝑍]𝑇 ，𝑃 ′ 点的坐标为[𝑋 ′ , 𝑌 ′ , 𝑍 ′ ]，并且设相机的成像平面到模
型中的针孔的距离为𝑓 （焦距）,则根据三角形的相似关系，有
𝑋
𝑌
𝑍
=− ′ =− ′
𝑓
𝑋
𝑌
28

(4-1)

第4章 基于视觉与三维点云融合的三维障碍物检测方法

图 4-4 相机针孔成像原理

整理得
⎧
𝑋
⎪
⎨ 𝑋′ = 𝑓
𝑍
(4-2)
⎪
⎩ 𝑌′=𝑓𝑌
𝑍
实际上，在相机中最后得到的是一个个的像素，设在相机成像平面上存在一
个像素平面𝑂 − 𝑢 − 𝑣，𝑃 ′ 在像素平面的坐标为[𝑢, 𝑣]𝑇 。像素坐标系通常定义其原
点𝑂′ 在图像的左上角，其𝑢轴与𝑣轴的方向与相机坐标系中的𝑋轴与𝑌 轴相同。像
素坐标系相对于相机坐标系，相差了一个平移与缩放。假设相机坐标系在𝑢轴上
缩放了𝛼倍，在𝑣轴上缩放了𝛽倍，同时像素坐标系相对于相机坐标系的原点平移
了[𝑐𝑥 , 𝑐𝑦 ]𝑇 。则𝑃 ′ 在成像平面上的坐标与其像素坐标[𝑢, 𝑣]𝑇 之间的关系为
⎧
⎨ 𝑢 = 𝛼𝑋 ′ + 𝑐𝑥
⎩ 𝑣 = 𝛽𝑌 ′ + 𝑐
𝑦
将上式代入式4-2中，并设𝛼𝑓 = 𝑓𝑥 ，𝛽𝑓 = 𝑓𝑦 ，则有
⎧
𝑋
⎪
⎨ 𝑢 = 𝑓 𝑥 + 𝑐𝑥
𝑍
⎪
⎩ 𝑣 = 𝑓 𝑦 𝑌 + 𝑐𝑦
𝑍

29

(4-3)

(4-4)

电子科技大学学士学位论文

将上式写为矩阵的形式，则有
⎡
⎤⎡ ⎤
⎡ ⎤
𝑓 𝑥 0 𝑐𝑥 𝑋
𝑢
⎥⎢ ⎥
⎢ ⎥ 1⎢
⎢𝑣 ⎥ = ⎢ 0 𝑓 𝑦 𝑐𝑦 ⎥ ⎢ 𝑌 ⎥ , 1 𝐾𝑃
(4-5)
⎦⎣ ⎦ 𝑍
⎣ ⎦ 𝑧⎣
0 0 1
𝑍
1
⎡
⎤
𝑓 𝑥 0 𝑐𝑥
⎢
⎥
⎥称之为相机的内参，通常，相机的标定就是为了得出相机
其中矩阵⎢
0
𝑓
𝑦
𝑐𝑦
⎣
⎦
0 0 1
的内参矩阵。本文在融合激光与相机之前已通过ROS的标定程序得到了相机的内
参𝐾。

4.2.2 激光雷达点云的投影
上式中，𝑃 为相机坐标系内的一点，所以在根据第二章得到的标定方法得出
激光雷达与相机坐标系的变换矩阵𝑇 后，先将激光雷达点云转换到相机坐标系中，
再通过内参矩阵投影到图像上，即
1
𝐾𝑇 𝑃𝐿𝑖𝐷𝐴𝑅
𝑍
其中𝑃𝐿𝑖𝐷𝐴𝑅 表示激光雷达坐标系中的点云中的三维点。
𝑃𝑢𝑣 =

(4-6)

图 4-5 激光雷达点云的投影

本文利用前文提到的三维感知机构进行了多帧点云的注册融合，并将融合后
的点云投影到了相机的图像上，点云投影效果如图4-5所示。其中投影点的颜色
30

第4章 基于视觉与三维点云融合的三维障碍物检测方法

越深，代表其距离相机的位置越近；颜色越浅，代表离相机的距离越远。在将激
光雷达点云投影到图像之后，便可以利用YOLO的检测结果对点云进行分割与分
类。

4.2.3 点云前景与背景的分割
本文首先利用YOLO对相机图像进行了目标检测的推断。YOLO的检测效果
如图4-6所示，其输出为多个边界框与识别的物体的类别，以及YOLO对推断结果
的置信度（confidence）。图4-6为设置YOLO检测的置信度阈值为0.8时输出的结果
（即不输出置信度低于0.8的检测结果）。

图 4-6 YOLO检测结果

将点云按照上文投影到图像中，则点云中的每个点𝑃 ′ 都有一个唯一的像素坐
标[𝑢, 𝑣]𝑇 与其对应。剔除像素坐标不在YOLO检测的边界框内的点，便得到了基
于YOLO检测结果的点云分割与分类结果，如图4-7(a)所示。
从图4-7(a)中可以看出，尽管按照上述结果的确将人与汽车的点云分割了出
来，然而由于YOLO得出的边界框内除了识别的目标，还有一些背景物体，体现

31

电子科技大学学士学位论文

(a)

(b)

图 4-7 点云前景与背景的分割(a)原始点云;(b)Kmeans分类后的结果

在点云中就是，除了表示人与轿车的点云，还有一些人与汽车后的灌木的点云，
其在图像上的投影也在候选框里。这些点云如果不加以去除，会对目标物体的三
维位置的确定造成较大的负面影响。
考虑到目标点云与背景点云转换到相机坐标系后，在Z轴方向上有较大的距
离，本文采用K-means算法[19] ，对投影后在同一边界框内的点云进行聚类分割。
K-means算法的目的是，把𝑛个点划分到𝐾个聚类中，使得每个点都属于离它
最近的均值（称之为聚类中心）对应的聚类，以之作为聚类标准。其算法流程
为：
1.假设分为K类；
2.任意随机出K个点，认为该K个点为聚类的中心点；
3.遍历每一个点𝑥，计算其与上一步得出的K个点的欧式距离𝐷(𝑥)，将该点归
为使得𝐷(𝑥)最小的聚类中；
4.计算每个聚类内所有点的平均位置，认为其为新的聚类中心；
5.重复3和4，直到𝐾个聚类中心被选出来；
6.利用得到的新的K个点，继续重复3-5的步骤，直到每个点的分类结果不变，
或者循环达到迭代次数上限。
对于图4-7(a)中的每个物体边界框内的点云，假设其可以被分为两类：前景与
背景。设原始三维点集为𝑃3 ，将所有点映射到相机坐标系的Z轴上，成为一个新
的一维点集𝑃1 ，并对𝑃1 进行K-means聚类，其中𝐾 = 2。则聚类得到的两类点中，

32

第4章 基于视觉与三维点云融合的三维障碍物检测方法

中心点Z轴坐标较小的即为前景，也就是YOLO检测的物体的点云。图4-7(b)为Kmeans聚类提取得到的结果。从图中可知，采用K-means算法较好的将前景与背景
分割开来，剔除了无关的背景点，只保留了与YOLO检测到的目标有关的点。

4.2.4 目标三维坐标的计算与检测结果的优化
在得到了与YOLO的检测结果相关的目标物体的点云后，物体的三维坐标可
以由求点集的中心点坐标而得。同时，为了更好的表示三维物体的检测结果，本
文还将检测得到的物体用长方体包围盒去拟合，如图4-9所示。包围盒的顶点由点
集的𝑋𝑚𝑖𝑛 , 𝑌𝑚𝑖𝑛 , 𝑍𝑚𝑖𝑛 以及𝑋𝑚𝑎𝑥 , 𝑌𝑚𝑎𝑥 , 𝑍𝑚𝑎𝑥 决定，其中𝑋𝑚𝑖𝑛 表示点集中𝑋轴坐标
最小的点的𝑋坐标，𝑋𝑚𝑎𝑥 表示点集中𝑋轴坐标最大的点的𝑋坐标。

图 4-8 YOLO在低置信度阈值下的误检测

前文提到， 图4-6中的结果是将YOLO检测结果的置信度阈值设为0.8后的检
测结果。而将YOLO检测结果的阈值设为0.4后，YOLO就出现了许多误检测，如
图4-8所示。由于光照等情况的影响，很多时候YOLO的检测结果置信度都不会高

33

电子科技大学学士学位论文

于0.8，而降低置信度阈值又会造成误检测的结果增多。本文提出了一种在YOLO
低置信度阈值下，融合激光雷达点云位置信息来祛除误检测结果的方法。
从图4-8中可以看出，大部分误检测的区域，其边界框对应的三维空间区域可
能没有点云，或者其三维位置信息与边界框面积不相符合。为了祛除误检测结果，
本文首先假设已知每个待检测的目标物体的最小投影矩形。举例来说，对图4-8来
说，认为汽车在相机坐标系中的最小投影矩形为2.4m×1.8m，即对于轿车而言，
至少有2.4m×1.8m的面积在相机视野范围内，那么，祛除误检测的策略流程可以
表述为：
1.如果YOLO检测得到的边界框内没有点云，则认为发生了误检测。
2.如果YOLO检测得到的边界框内有点云，则计算点云中所有点的平均坐标
𝑋𝑚𝑒𝑎𝑛 , 𝑌𝑚𝑒𝑎𝑛 , 𝑍𝑚𝑒𝑎𝑛 。
3.在相机坐标系中构建长方形平面，其顶点坐标为
(𝑋𝑚𝑒𝑎𝑛 − 𝑋ℎ𝑦𝑝𝑜 , 𝑌𝑚𝑒𝑎𝑛 − 𝑌ℎ𝑦𝑝𝑜 , 𝑍𝑚𝑒𝑎𝑛 )
(𝑋𝑚𝑒𝑎𝑛 + 𝑋ℎ𝑦𝑝𝑜 , 𝑌𝑚𝑒𝑎𝑛 − 𝑌ℎ𝑦𝑝𝑜 , 𝑍𝑚𝑒𝑎𝑛 )

(4-7)

(𝑋𝑚𝑒𝑎𝑛 − 𝑋ℎ𝑦𝑝𝑜 , 𝑌𝑚𝑒𝑎𝑛 + 𝑌ℎ𝑦𝑝𝑜 , 𝑍𝑚𝑒𝑎𝑛 )
(𝑋𝑚𝑒𝑎𝑛 + 𝑋ℎ𝑦𝑝𝑜 , 𝑌𝑚𝑒𝑎𝑛 + 𝑌ℎ𝑦𝑝𝑜 , 𝑍𝑚𝑒𝑎𝑛 )
其中，𝑋ℎ𝑦𝑝𝑜 , 𝑌ℎ𝑦𝑝𝑜 为上文提到的最小投影矩阵的长与宽。
4.将该长方形平面通过内参矩阵投影到相机图像中，得到一个估计的长方形
框选面积𝑆ℎ𝑦𝑝𝑜 。
5.将𝑆ℎ𝑦𝑝𝑜 与YOLO检测出的边界框的面积𝑆𝑦𝑜𝑙𝑜 做比较，认为不满足约束条件
0.5 × 𝑆ℎ𝑦𝑝𝑜 ≤ 𝑆𝑦𝑜𝑙𝑜 ≤ 1.5 × 𝑆ℎ𝑦𝑝𝑜

(4-8)

的检测结果为误检测。
经过该点云图像融合祛除误检测策略后，三维物体检测结果如图4-9所示。其
中误检测的部分用红色矩形标出，而正确的检测部分利用上文提出的长方体包围
盒表示出来。可以看出，通过该策略能够有效的排除YOLO在低置信度阈值下的
误检测，并且利用激光雷达的信息，能够很好的计算出检测的目标的三维位置。

4.3 本章小结
本章介绍了YOLO，一种实时的视觉目标检测方法，该方法最大的优点在于
其实时性，能够在较高的识别准确率下，达到每秒推断45帧的速度。并且之后融
34

第4章 基于视觉与三维点云融合的三维障碍物检测方法

合了相机与激光雷达的信息，对三维障碍物进行了分割与分类。首先将激光雷达
的点云信息投影到图像上，利用YOLO 的推断结果，首先对激光雷达点云做了前
景的提取与分割；随后，根据点云信息，计算得出了检测的目标的三维位置信息，
并计算出物体的三维包围盒；最后，针对YOLO在低置信度阈值下容易出现误检
测的问题，本文利用点云信息来验证YOLO的推断结果，并且甄别与祛除了误检
测的输出。从结果中可以看出，融合视觉与三维点云的信息，能够较好的对三维
物体进行检测与识别，为无人驾驶技术中的环境感知与路径规划提供了更多的信
息。

图 4-9 enhanced YOLO

35

电子科技大学学士学位论文

第5章 实验验证与结果分析

本章将对文中提出的三维感知机构的进行仿真，对融合后的点云的注册融合
效果进行了实验分析，并且融合了Odometry的信息。

5.1 Gazebo下的三维感知机构仿真
Gazebo是一个功能强大的三维物理仿真平台， 具备强大的物理引擎、 高质
量的图形渲染、方便的编程与图形接口，最重要的还有其具备开源免费的特性。
Gazebo支持显示逼真的三维环境，包括光线、纹理、影子等。它还支持传感器数
据的仿真，同时可以仿真传感器噪声。
本文利用Gazebo对三维感知机构的运动以及点云融合进行了仿真，如图5-1(a)
以及图5-1(b)所示。

(a)

(b)

图 5-1 Gazebo仿真(a)三维感知机构的模型;(b)仿真环境

三维感知机构的Gazebo模型如图5-1(a)所示。由于在加上了ROS control的控制
器后，利用单个圆柱形的link连接雷达，控制link做旋转运动就能够使得激光雷达
在航向角上做来回的正弦运动，而进行Gazebo仿真的目的主要是为了仿真出激光
雷达在往复运动情况下的点云注册融合情况，因此本文没有在Gazebo中设计曲柄
摇杆机构。在仿真实验中，激光雷达传感器使用的是Velodyne 的VLP-16型激光雷

36

第5章 实验验证与结果分析

达。Gazebo支持传感器数据的仿真，按照第三章的注册融合策略融合后的点云如
图5-2所示。

图 5-2 Gazebo仿真得到的融合点云

从图中可以看出，经仿真得到的点云基本能够反应仿真环境下障碍物的分布
情况。

5.2 三维感知机构结合里程计信息构建三维地图
前文提出的三维感知机构，其实验场景都是在机构位置静止不变的情况下得
到的。而当机构架设在小车上时，由于小车相对于世界坐标系有一个运动，如果
仍然采用之前的融合策略进行点云的输出，则输出的点云会有小车运动方向上的
畸变。
因此，在运动的小车上进行三维感知时，需要结合里程计的信息，计算出机
构在小车运动方向上的位移，借此消除点云在小车运动方向上的畸变。同时，可
以根据小车自身坐标系到里程计坐标系的变换，利用该三维感知机构构建基于小
车里程计坐标系的三维地图。
本文结合图4-3所示的传感器设置，利用小车的里程计对小车行驶过程中的
点云进行了采集、融合与构建了三维地图。本文利用了ROS的TF坐标变换[20] 的设
计，在结合里程计信息的同时，避免了繁琐的插值运算。
ROS的TF是一种这样的设计：TF是一个让使用者随时间跟踪多个坐标系的功
能包。其数据结构类型为树状结构，能够根据时间缓冲并维护多个坐标系之间的
坐标变换关系，可以帮助使用者在任意的时间点请求任意的坐标系之间的变换。
37

电子科技大学学士学位论文

在本章的实现中，首先认为机构自身的坐标系为𝑏𝑎𝑠𝑒𝑙𝑖𝑛𝑘，而激光雷达所在
的坐标系为𝑙𝑖𝑑𝑎𝑟，根据磁编码器返回的角度𝛼，发布𝑏𝑎𝑠𝑒𝑙𝑖𝑛𝑘坐标系到𝑙𝑖𝑑𝑎𝑟坐标
系的变换，认为其只有绕y轴旋转，即偏航角的变化，而机构的往复运动没有造成
两个坐标系之间的位移，故其欧式变换矩阵为
⎡
⎤
cos 𝛼 0 sin 𝛼 0
⎢
⎥
⎢ 0
⎥
1
0
0
⎣
⎦
− sin 𝛼 0 cos 𝛼 0
随后，根据小车底层的编码器的信息，发布𝑜𝑑𝑜𝑚坐标系到𝑏𝑎𝑠𝑒𝑙𝑖𝑛𝑘坐标系的
变换。该欧式变换的旋转与位移皆通过小车的编码器与阿克曼转角的角度积分得
到。

图 5-3 结合里程计生成的三维地图

那么在TF树中，𝑜𝑑𝑜𝑚坐标系的子节点为𝑏𝑎𝑠𝑒𝑙𝑖𝑛𝑘坐标系，𝑏𝑎𝑠𝑒𝑙𝑖𝑛𝑘坐标系的
子节点为𝑙𝑖𝑑𝑎𝑟坐标系。则当激光雷达点云产生时，可以根据激光雷达的产生的点
云信息的时间戳𝑡来请求𝑜𝑑𝑜𝑚坐标系到𝑏𝑎𝑠𝑒𝑙𝑖𝑛𝑘坐标系之间的欧式变换。尽管有可
能在时间戳𝑡上没有发布𝑜𝑑𝑜𝑚到𝑏𝑎𝑠𝑒𝑙𝑖𝑛𝑘、𝑏𝑎𝑠𝑒𝑙𝑖𝑛𝑘到𝑙𝑖𝑑𝑎𝑟坐标系的欧式变换，但
是ROS的TF可以利用在时间戳𝑡前后时刻已发布的欧式变换来进行插值得出𝑡时刻
的变换矩阵。有了𝑡时刻的变换矩阵，可以很容易的将激光雷达坐标系下的点云变
换到世界坐标系中。将每帧点云在世界坐标系中进行注册，则可得基于三维感知

38

第5章 实验验证与结果分析

机构生成的三维点云地图，如图5-3所示。

5.3 基于点云投影到深度图像上的物体分割
在得到了基于三维感知机构融合与注册的点云后，一个重要的步骤就是进行
点云的分割，进而求得点云中可能为障碍物的部分。本章节的实验环境如图5-4所
示。

图 5-4 实验环境

在该实验中，首先进行三维感知传感器机构对点云的融合，融合结果如图55(a)所示。关于点云注册融合的细节，本文已经在第三章中详细阐述，此处便不
再赘述。

5.3.1 地面点的去除
有关点云的聚类，实际上就是指将三维空间点中的相近的点认为是同一个物
体，将其归为同一类。而从点云图像中可以看出，地面点构成了点云的绝大部分。
在聚类的过程中，地面点会对聚类的结果造成较大的影响，体现在地面点将两个
不同物体的点云连接了起来，从而使得聚类算法会将两个不同的物体归为一类。
本文采用文献[21] 提出的算法，首先对地面进行平面拟合，求出近似平面的方程，
随后根据方程将平面方程以下的点去除。
39

电子科技大学学士学位论文

(a)

(b)

图 5-5 多帧融合后的点云(a)原始点云;(b)去除地面点后的点云

在去除地面点之前，该算法首先提出了两个假设：
1.认为地面可以被近似为一个平面，即认为环境中地面是较为平整而不是有
曲率的。
2.认为地面点永远是点云中高度最低的一些点。
算法流程如Algorithm 2所示。该算法使用最低点作为代表平面的点（lowest
point repersentative）。首先，该算法先提取地面点的“种子（seed）”点，即认为
最低的𝑁𝐿𝑃 𝑅 个点中，高度为平均值再加一个阈值为𝑇 ℎ𝑠𝑒𝑒𝑑𝑠 的点为地面点。这个
阈值有效的预防了传感器噪声的影响，即避免了因传感器噪声而导致的过低的点
对地面的平面拟合造成影响。
为了估计地面的方程，该算法使用了一个简单的线性方程：
𝑎𝑥 + 𝑏𝑦 + 𝑐𝑧 + 𝑑 = 0
𝑛𝑇 𝑥 = −𝑑

(5-1)

其中，𝑛 = [𝑎, 𝑏, 𝑐]𝑇 ，𝑋 = [𝑥, 𝑦, 𝑧]𝑇 ，该方程首先求得关于种子点的协方差矩阵，通
过协方差矩阵求得平面的法向量𝑛。协方差矩阵𝐶通过下式
∑︁
𝐶=
(𝑠𝑖 − 𝑠^)(𝑠𝑖 − 𝑠^)𝑇

(5-2)

𝑖=1:|𝑆|

而得。其中^
𝑠 ∈ 𝑅3 是所有𝑠𝑖 ∈ 𝑆的平均值。
协方差矩阵𝐶表征了种子点的离散程度，通过对其进行奇异值分解（singular
value decomposition）可得其奇异向量（singular vectors），奇异向量描述了其在三
40

第5章 实验验证与结果分析

个方向上的离散程度。考虑到地面较为平坦，其种子点在竖直方向上的离散程度
比较小，则与地面垂直的法向量𝑛即为三个奇异向量中模最小的一个。
在得到法向量𝑛之后，在式5-1中，令𝑥 = 𝑠^，求得𝑑。如此便得到了对地面进行
拟合的平面方程。对于点云中的每个点𝑝(𝑥, 𝑦, 𝑧)，求𝑛𝑇 𝑥 + 𝑑的值，若大于0，则在
平面之上（为非地面点）；若小于0，则在平面之下（为地面点）。

5.3.2 点云到深度图像的投影
正常的16线激光雷达点云每帧拥有三万个三维点，而由于本文提到的三维感
知机构融合注册了多帧点云，其发布的点云每帧高达几十万个三维点。如此巨大
的数据规模，如果采用常规的三维点云聚类方法，其每帧耗时将相当可观（在三
维点云上的聚类算法通常时间复杂度在O(nlog(n))以上）。因此需要一些对数据进
行降采样的方法来提高算法的效率。
第二种方法是将去除地面点后的三维点云投影到地面的栅格平面上，这种方
法也称为生成点云的鸟瞰图（bird’s eye view）。随后在二维图像上进行物体的分
割。这种方法运算速度很快，适合实施运算。然而这种方法有可能不能充分分割
障碍物，如果多个物体彼此比较接近，它们有可能被认为是同一个物体。这取决
于给地面划分栅格时栅格的大小，所以在不同的环境下，有可能要调整不同的栅
格大小来改进算法。

图 5-6 三维点的球坐标表示

41

电子科技大学学士学位论文

而本文参考文献[22] 提出的方法，将点云投影到深度图像（Range Image）上，
并且不需要在深度图像中提取特征，而是通过一种图搜索的算法将物体进行分割
与定位。也因此，其时间复杂度为O(n)级别，能够满足实时性的要求。
将点云投影为深度图像，首先要将点云中的点的表示由笛卡尔坐标系转换为
球坐标系，如图5-6所示。球坐标系中的三维点𝑝 = (𝛼, 𝜔, 𝑟)，其中𝛼与𝛽的表示如图
所示，𝑟为三维点到坐标系原点的距离。则显然，点的球坐标系与笛卡尔坐标系坐
标转换的关系为
⎧
√︀
⎪
⎪
𝑟
=
𝑥2 + 𝑦 2 + 𝑧 2
⎪
⎪
⎨
𝜔 = arcsin 𝑧𝑟
⎪
⎪
⎪
⎪
⎩𝛼 = arccos 𝑦
𝑟 cos 𝜔

(5-3)

求得三维点的球坐标后，就要根据三维点的𝛼与𝜔来进行到深度图像的投影。
假设深度图像的长与宽为𝑤与ℎ，并且空间中有一三维点𝑝3𝑑 = (𝛼, 𝜔, 𝑟)，首先设融
合后的点云中的点的最大𝛼角为𝛼𝑚𝑎𝑥 ，最大𝜔角为𝜔𝑚𝑎𝑥 ，则三维点𝑝3𝑑 映射到二维
点𝑝2𝑑 = (𝑢, 𝑣)的关系为
⎧
⎪
⎪
𝛼𝑝𝑒𝑟𝑐𝑜𝑙 = 2𝛼𝑚𝑎𝑥 /𝑤
⎪
⎪
⎪
⎪
⎪
⎨𝜔𝑝𝑒𝑟𝑟𝑜𝑤 = 2𝜔𝑚𝑎𝑥 /ℎ

(5-4)

⎪
⎪
𝑢 = (𝛼/𝛼𝑝𝑒𝑟𝑐𝑜𝑙 + 𝑤/2) mod 𝑤
⎪
⎪
⎪
⎪
⎪
⎩𝑣 = −𝜔/𝜔
𝑝𝑒𝑟𝑟𝑜𝑤 + ℎ/2
其中𝛼𝑝𝑒𝑟𝑐𝑜𝑙 与𝜔𝑝𝑒𝑟𝑟𝑜𝑤 是深度图像中每行与每列代表的角度，而有关𝑢, 𝑣的变换是为
了让激光雷达坐标系中正前方的点云能够投影到深度图像的中心。经过投影后得
到的深度图像如图5-7所示，其中像素点的像素值为三维点的𝑥坐标值，像素值越
大，颜色越亮，表示该像素所表征的三维点离相机越远。

图 5-7 由点云投影得到的深度图像

42

第5章 实验验证与结果分析

Algorithm 2 ground plane fitting methodology for one segment of the point cloud
Output:
𝑃𝑔 : points belonging to ground surface
𝑃𝑛𝑔 :points not belonging to ground surface
1:

Initialization:

2:

𝑃 : input point cloud

3:

𝑁𝑖𝑡𝑒𝑟 : number of iterations

4:

𝑁𝐿𝑃 𝑅 : number of points used to estimate the LPR

5:

𝑇 ℎ𝑠𝑒𝑒𝑑𝑠 : threshold for points to be considered initial seeds

6:

𝑇 ℎ𝑑𝑖𝑠𝑡 : threshold distance from the plane

7:

Main Loop:

8:

𝑃𝑔 = ExtractInitialSeeds(P, 𝑁𝐿𝑃 𝑅 , 𝑇 ℎ𝑠𝑒𝑒𝑑𝑠 );

9:

for 𝑖 = 1 : 𝑁𝑖𝑡𝑒𝑟 do

10:

model = EstimatePlane(𝑃𝑔 );

11:

clear(𝑃𝑔 , 𝑃𝑛 𝑔);

12:

for 𝑘 = 1 : ‖𝑃 ‖ do

13:
14:
15:

if 𝑚𝑜𝑑𝑒𝑙(𝑝𝑘 ) < 𝑇 ℎ𝑑𝑖𝑠𝑡 then
𝑃𝑔 ← 𝑝𝑘 ;
else

16:

𝑃𝑛𝑔 ← 𝑝𝑘 ;

17:

ExtractInitialSeeds:

18:

𝑃𝑠𝑜𝑟𝑡𝑒𝑑 =SortOnHeight(𝑃 );

19:

𝐿𝑃 𝑅=Average(𝑃𝑠𝑜𝑟𝑡𝑒𝑑 (1 : 𝑁𝐿𝑃 𝑅 ));

20:

for 𝐾 = 1 : ‖𝑃 ‖ do

21:
22:

if 𝑝𝑘 .ℎ𝑒𝑖𝑔ℎ𝑡 < 𝐿𝑃 𝑅.ℎ𝑒𝑖𝑔ℎ𝑡 + 𝑇 ℎ 𝑠𝑒𝑒𝑑𝑠 then
𝑠𝑒𝑒𝑑𝑠 ← 𝑝𝑘 ;
return 𝑠𝑒𝑒𝑑𝑠

43

电子科技大学学士学位论文

5.3.3 基于深度图像的物体分割
投影后的深度图像相当于将具有相同𝛼, 𝜔而𝑟不同的点用同一个位置的像素点
来表示，因而相当于通过损失有限的信息对点云信息进行了压缩。在实际无人车
的行驶情况中，由于当𝛼, 𝜔相同时，可以先关注𝑟比较小的点，这些点表示同一方
向上近处的物体，这对无人驾驶时的路径规划是十分重要的，因而这样的信息损
失是完全可以接受的。
对于深度图像上的物体分割，本文采用广度优先搜索（Breadth First Search，
BFS）的思想，对深度图像中每个像素值不为0的点，搜索其邻域内的点，若其像
素值（即实际的距离）与当前像素值之差不超过某个阈值，则认为其为同一类物
体。算法流程如Algorithm 3所示。

Algorithm 3 Range Image Labelling
1: LabelRangeImage(R)
2:

Label ← 1, 𝐿 ← 𝑧𝑒𝑜𝑟𝑠(𝑅𝑟𝑜𝑤𝑠 × 𝑅𝑐𝑜𝑙𝑠 )

3:

for 𝑟 = 1 : 𝑅𝑟𝑜𝑤𝑠 do

4:
5:

for 𝑐 = 1 : 𝑅𝑐𝑜𝑙𝑠 do
if 𝐿(𝑟, 𝑐) == 0 then

6:

LabelComponentBFS(𝑟, 𝑐,Label);

7:

Label= Label+1;

8:
9:
10:

LabelComponentBFS(𝑟, 𝑐,Label)
queue.push({𝑟, 𝑐})
while queue is not empty do

11:

{𝑟, 𝑐}=queue.top();

12:

𝐿(r,c)=Label

13:

for {𝑟𝑛 , 𝑐𝑛 }∈Neighbourhood{𝑟, 𝑐} do

14:
15:

if 𝑎𝑏𝑠(𝑅(𝑟𝑛 , 𝑐𝑛 ) − 𝑅(𝑟, 𝑐)) < 𝑇 ℎ𝑟𝑒𝑠 then
queue.push({𝑟𝑛 , 𝑐𝑛 })
queue.pop()
在实际的实现中，𝑇 ℎ𝑟𝑒𝑠取值为0.5，分割的结果如图5-8所示。本文将每个不

同聚类的物体标记为不同的颜色，从图中可以看出，这种方法很好的将空间位置
不同的物体分割了出来。
44

第5章 实验验证与结果分析

在深度图像上将物体分割后，一个重要的步骤就是根据图像中物体的像素坐
标以及深度值反推出物体在世界坐标系中的坐标。假设该物体所有像素的平均坐
标为𝑝(𝑢, 𝑣)，其平均深度通过求所有像素的平均值而得，设为𝑟。则可以根据三维
点云投影深度图的算法倒推得其三维坐标：
⎧
⎪
𝛼 = (𝑢 − 𝑤/2) * 𝛼𝑝𝑒𝑟𝑐𝑜𝑙
⎪
⎪
⎪
⎪
⎪
⎪
⎪
𝜔 = −(𝑣 − ℎ/2) * 𝜔𝑝𝑒𝑟𝑟𝑜𝑤
⎪
⎪
⎨
𝑥 = 𝑟 cos(𝛼) cos(𝜔)
⎪
⎪
⎪
⎪
⎪
⎪
𝑦 = −𝑟 cos(𝛼) sin(𝜔)
⎪
⎪
⎪
⎪
⎩
𝑧 = 𝑟 sin(𝜔)

(5-5)

图 5-8 基于深度图像的聚类分割结果

由此，便实现了基于点云投影到深度图像上的实时物体分割与定位。

5.4 本章小结
本章主要对本文第三章提出的三维感知机构的点云融合注册的策略进行了
Gazebo 仿真验证，同时拓展了融合后的点云的应用，将多帧融合后的点云应用
于三维稠密地图的构建以及投影到深度图像中进行分割。在Gazebo仿真实验中可
以发现第三章提出的融合策略有效地将点云进行了融合注册，融合后的点云能够
较为清晰地反应仿真环境中障碍物的位置与形状；而将三维感知机构的信息与无
人车里程计的信息结合后，又能够将多帧融合后的点云用于大规模三维建图之中，
准确地反映出对环境的感知；最后，利用多帧融合后的点云投影到深度图像中，
并对深度图像进行聚类分割，能够实现无人车环境下的障碍物分割与定位，对无
人车环境下的环境感知与自主导航具有较大的意义。

45

电子科技大学学士学位论文

第6章 全文总结与后续工作展望
6.1 全文总结
在无人车自动驾驶领域，如何利用好激光雷达来进行环境感知与障碍物检测
一直是一个热门的话题。低线数激光雷达在环境感知时返回的信息较少，很难直
接根据点云信息对物体进行识别，而高线数激光雷达又造价高昂，极大限制了无
人驾驶的成本。因此，本文立足于解决低线数激光雷达由于线数较少而较难根据
点云信息检测障碍物的问题，设计了一种新颖的三维感知机构，能够通过给低线
数激光雷达提供航向角上的往复运动来增加激光雷达竖直方向上的分辨率；同时，
本文亦融合了相机与激光雷达点云的数据信息，利用图像对障碍物进行了分类与
分割，结合点云完成了对障碍物的定位。本文完成的主要研究工作有：
1.提出一种新颖的三维感知机构，该机构通过无刷电机驱动曲柄摇杆机构来
给底座上的激光雷达提供航向角上的往复运动，并通过融合多帧激光雷达
的点云来增加低线数激光雷达在竖直方向上的点云的分辨率。在融合的过
程中，本文通过帧内多次线性插值来矫正因激光雷达往复运动而造成的运
动畸变，使得融合后的点云能够更加真实地反应环境的情况。
2.融合了相机图像与激光雷达点云的信息，对三维障碍物进行了分割、分类
以及定位。首先利用目标检测网络YOLO来对图像进行推断，对障碍物进行
了分割与分类；随后，根据投影到相机上的点云信息，计算出了待检测目
标的三维位置信息；最后，利用点云的三维位置信息来解决YOLO在低置信
度阈值下的误检测问题，提高了YOLO在低执行度阈值下的目标检测的查准
率。
综上，本文面向无人车领域，提出了一种新颖的利用低线数激光雷达多帧融
合来进行环境感知的方式。通过融合多帧点云，本文提出的机构能够提供类似于
高线数激光雷达的丰富的点云信息，同时利用该点云信息，结合相机图像信息进
行了三维障碍物的识别与定位。

46

第6章 全文总结与后续工作展望

6.2 后续工作展望
有关无人车自动驾驶环境感知的研究近几年发展迅速，在本文研究工作的基
础上，仍有以下方向值得进一步研究：
1.在本文提出的多帧融合三维感知机构中，如果障碍物运动速度较快，则由
于多帧融合，在融合后的点云中会有由于障碍物运动而产生的拖影。在三
维建图时，如何消除运动物体产生的点云来建立只有静态物体的地图是一
个值得深入研究的问题。一个可行的办法是引入八叉树地图（OctoMap），
对于传感器观测到的障碍物，归入八叉树地图，利用贝叶斯滤波来滤除动
态物体；随后再用八叉树地图来恢复出没有动态障碍物的点云。
2.在相机与传感器的融合中，目前使用的YOLO卷积网络的输入为图像，其
为𝑚 × 𝑛 × 3的有序矩阵信息。而通过将融合后的点云投影，则能够为图像
的像素提供第四维信息，即深度信息，如果将四维像素信息输入卷积网络，
则有希望大大提高YOLO目标检测的查准率与查全率。而且由于本文所得的
激光雷达进行了竖直方向的分辨率的提升，使得投影到图像上的点云密度
能够较好的和像素相匹配。
总而言之，本文提出的三维感知机构实现了低线数激光雷达点云在竖直方向上的
稠密化，其对于三维稠密地图的建立以及三维目标检测的任务都有较为重要的意
义，基于此平台可以未来可以开展许多有关三维环境感知感知、三维建图等方面
的工作。

47

电子科技大学学士学位论文

参考文献

[1] F. Li. Imagenet Large Scale Visual Recognition Challenge (ILSVRC)[EB/OL]. http://www.
image-net.org/challenges/LSVRC/, Dec 16, 2010
[2] T. Rabbani, F. Van Den Heuvel. Efficient hough transform for automatic detection of cylinders in
point clouds[J]. Isprs Wg Iii/3, Iii/4, 2005, 3:60–65
[3] M. Montemerlo, J. Becker, S. Bhat, et al. Junior: The stanford entry in the urban challenge[J].
Journal of field Robotics, 2008, 25(9):569–597
[4] J. Behley, V. Steinhage, A. B. Cremers. Laser-based segment classification using a mixture of
bag-of-words[C]. 2013, 4195–4200
[5] D. Korchev, S. Cheng, Y. Owechko, et al. On real-time lidar data segmentation and classification[C]. 2013, 1
[6] Y. Zhou, O. Tuzel. Voxelnet: End-to-end learning for point cloud based 3d object detection[C].
2018, 4490–4499
[7] F. Zhang, D. Clarke, A. Knoll. Vehicle detection based on lidar and camera fusion[C]. 2014,
1620–1625
[8] J. Zhang, S. Singh. LOAM: Lidar Odometry and Mapping in Real-time.[J]. 2014, 2:9
[9] 王良才等. 机械设计基础[M]. 北京: 北京大学出版社, 2007, 72–73
[10] S. Hong, H. Ko, J. Kim. VICP: Velocity updating iterative closest point algorithm[C]. 2010,
1893–1898
[11] A. Dhall, K. Chelani, V. Radhakrishnan, et al. LiDAR-camera calibration using 3D-3D point
correspondences[J]. arXiv preprint arXiv:1705.09785, 2017
[12] S. Garrido-Jurado, R. Muñoz-Salinas, F. J. Madrid-Cuevas, et al. Automatic generation and
detection of highly reliable fiducial markers under occlusion[J]. Pattern Recognition, 2014,
47(6):2280–2292
[13] V. Lepetit, F. Moreno-Noguer, P. Fua. EPnP: An Accurate O(n) Solution to the PnP Problem[J].
International Journal of Computer Vision, 2008, 81(2):155
[14] J. Bacik. aruco mapping[EB/OL]. http://wiki.ros.org/aruco_mapping, Dec 16, 2016
[15] Z. Zhang. Iterative point matching for registration of free-form curves and surfaces[J]. International Journal of Computer Vision, 1994, 13(2):119–152
48

参考文献
[16] W. Kabsch. A solution for the best rotation to relate two sets of vectors[J]. Acta Crystallographica Section A: Crystal Physics, Diffraction, Theoretical and General Crystallography, 1976,
32(5):922–923
[17] O. Sorkine. Least-squares rigid motion using svd[J]. Technical notes, 2009, 120(3):52
[18] J. Redmon, S. Divvala, R. Girshick, et al. You only look once: Unified, real-time object detection[C]. 2016, 779–788
[19] J. MacQueen, et al. Some methods for classification and analysis of multivariate observations[C].
1967, 281–297
[20] T. Foote. tf: The transform library[C]. 2013, 1–6
[21] D. Zermas, I. Izzat, N. Papanikolopoulos. Fast segmentation of 3D point clouds: A paradigm on
LiDAR data for autonomous vehicle applications[C]. 2017
[22] I. Bogoslavskyi, C. Stachniss. Efficient online segmentation for sparse 3d laser scans[J]. PFG–
Journal of Photogrammetry, Remote Sensing and Geoinformation Science, 2017, 85(1):41–52
[23] X. Han, J. Lu, Y. Tai, et al. A real-time LIDAR and vision based pedestrian detection system for
unmanned ground vehicles[C]. 2015, 635–639

49

电子科技大学学士学位论文

致 谢


