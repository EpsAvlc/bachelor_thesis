摘要

摘 要

das
关键词：无人驾驶，激光雷达，障碍物检测，深度学习，多传感器融合

I

ABSTRACT

ABSTRACT

With the widespread engineering applications ranging from broadband signals and
non-linear systems, time-domain integral equations (TDIE) methods for analyzing transient electromagnetic scattering problems are becoming widely used nowadays. TDIEbased marching-on-in-time (MOT) scheme and its fast algorithm are researched in this
dissertation, including the numerical techniques of MOT scheme, late-time stability of
MOT scheme, and two-level PWTD-enhanced MOT scheme. The contents are divided
into four parts shown as follows.
……
Keywords: time-domain electromagnetic scattering, time-domain integral equation
(TDIE), marching-on in-time (MOT) scheme, late-time instability, plane
wave time-domain (PWTD) algorithm

III

目录

目 录

第1章 绪论 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1

1.1 研究工作的背景与意义 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1

1.2 无人车三维障碍物检测的国内外研究历史与现状 . . . . . . . . . . . . . . . . . . . .

2

1.3 本文的主要贡献与创新 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2

1.4 本论文的结构安排 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2

第2章 三维感知传感器机构设计 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3

2.1 三维感知传感器机构的机械结构设计 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3

2.1.1 机械结构运动原理 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4

2.1.2 曲柄连杆机构的设计 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5

2.2 三维感知传感器机构的电路设计 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7

2.2.1 驱动器 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7

2.2.2 执行机构 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8

2.2.3 传感器 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8

2.2.3.1 角度传感器 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8

2.2.3.2 激光雷达 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8

2.2.4 主控板 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8

2.2.5 电路拓扑 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9

2.3 三维感知传感器机构的软件设计与运动控制 . . . . . . . . . . . . . . . . . . . . . . . .

9

2.4 本章小结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

11

第3章 点云的多帧融合与激光雷达和相机标定 . . . . . . . . . . . . . . . . . . . . . . . . . . . .

12

3.1 激光雷达点云的多帧融合 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

12

3.1.1 一种朴素的多帧融合策略 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

12

3.1.2 点云的运动畸变的形成与矫正 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

13

3.1.2.1 点云的运动畸变 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

13

3.1.2.2 运动畸变的矫正 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

14

3.1.3 矫正运动畸变后的多帧融合策略 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

16

3.2 激光雷达与相机的标定 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

17

V

目录

3.2.1 标定板 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

17

3.2.2 相机坐标系中的三维特征点提取 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

18

3.2.3 LiDAR坐标系中的三维特征点提取 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

19

3.2.4 刚体变换求解 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

19

3.2.5 多帧刚体变换结合 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

22

3.3 本章小结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

22

第4章 基于视觉与三维点云融合的三维障碍物检测方法 . . . . . . . . . . . . . . . . . . .

24

4.1 YOLO-一种实时目标检测网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

24

4.2 基于YOLO的视觉、三维点云结合的三维障碍物检测 . . . . . . . . . . . . . . . . .

26

4.2.1 相机成像原理 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

26

4.2.2 激光雷达点云的投影 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

28

4.2.3 点云前景与背景的分割 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

29

4.2.4 目标三维坐标的计算与检测结果的优化 . . . . . . . . . . . . . . . . . . . . . . . . .

31

4.3 本章小结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

32

第5章 实验验证与结果分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

34

5.1 Gazebo下的三维感知机构仿真 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

34

5.2 三维感知机构结合里程计信息构建三维地图 . . . . . . . . . . . . . . . . . . . . . . . .

34

5.3 后续工作展望 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

35

第6章 全文总结与后续工作展望 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

36

6.1 全文总结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

36

6.2 后续工作展望 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

36

参考文献 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

37

致 谢 .................................................................

38

外文资料原文 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

39

外文资料译文 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

41

VI

第1章 绪论

第1章 绪论
1.1 研究工作的背景与意义
近几年来，自动驾驶技术取得了长足的进步，而其中关键的技术就是多传感
器的环境感知与融合。环境感知的一个重要环节便是障碍物检测。目前，虽然基
于图像的障碍物检测已经取得了卓有成效的进步，然而相较于三维障碍物检测，
二维障碍物检测有以下缺陷：
1.基于单目相机的障碍物检测没有尺度信息，无法恢复出目标的三维坐标。
2.基于双目相机的障碍物检测，当基线较短时，测量距离较长（5m以上）的
物体时计算出来的距离信息很不准确，而当基线较长时，近处物体的检测
又容易出现在两个相机的视野盲区之中，从而导致无法三角化而得出距离
信息。
基于上述原因，越来越多的目光聚焦在了基于激光雷达（LiDAR）的三维障
碍物检测。LiDAR是Light Detection And Ranging的缩写，中文译作“激光探测与
测量”，一般指多线数的三维激光雷达传感器。相较于相机图像，激光雷达的点
云拥有以下几点优势：
1.测量范围广。目前的激光雷达的测量有效距离基本都在0.5-100米左右，远
高于双目相机三角测距的适用范围。
2.测量精度高。激光雷达的测距误差可达厘米级，同样优于双目相机的测距
结果。
目前最常见的旋转式激光雷达，其本质是多个激光束旋转后对每个时刻的测
距结果进行保留与叠加，最后再以点云的形式发布出去。决定激光雷达的分辨率
的一个参数为其激光束的个数，一般称之为激光雷达的线数。目前常用的激光雷
达线数有16线、32线、64线等，其中由于低线数的激光雷达生成的点云在测量远
距离物体时密度较低。举例来说，当使用16线激光雷达检测到20米处的障碍物时，
其16线激光束两两之间的距离可以达到70cm，相当于检测20m处的人时，只能够
有两线激光束能够返回距离。因此，低线数激光雷达点云的稀疏性较大地制约了
三维障碍物检测任务的准确率。

1

电子科技大学学士学位论文

通常在自动驾驶的无人车系统中会在车的四周装上多个16线的激光雷达进行
点云融合，或者直接采用线数更高的激光雷达来做障碍物检测的任务。然而目前
三维激光雷达造价不菲，无人车系统中光是64线激光雷达的成本就奖金十万美金，
如此高昂的成本在一定程度上限制了无人驾驶汽车的普及与推广。
鉴于上述存在在问题，本文希望能够提出一种基于多帧融合的低线数激光雷
达感知机构，使其能够通过增加一个在垂直方向的往复运动，并将该机构上激光
雷达的多帧点云融合发布来提高三维点云的稠密性，借而解决低线数激光雷达在
障碍物检测问题上由于点云的稀疏性而造成的困难。并且，本文还希望通过融合
上述机构发布的点云信息以及相机的图像信息，发挥各个传感器的优势从而提高
三维障碍物检测任务的准确率与实现三维障碍物的多类别检测。

1.2 无人车三维障碍物检测的国内外研究历史与现状
根据本文的主要研究方向，下面将对基于激光雷达点云的三维目标分割与检
测（3D object segmentation and detection）的研究现状进行调研。
针对三维障碍物的分割问题，目前主要有三类方法来实现

1.3 本文的主要贡献与创新
1.4 本论文的结构安排

2

第2章 三维感知传感器机构设计

第2章 三维感知传感器机构设计

为了能够解决低线数激光雷达在障碍物检测问题上由于点云的稀疏性而造成
的困难，本章提出了一种三维感知传感器机构，通过增加三维激光雷达在垂直方
向的往复旋转，同时融合多帧激光点云，来增加激光雷达在铅垂方向上的分辨率，
实现类似于高线数激光雷达的稠密点云。

2.1 三维感知传感器机构的机械结构设计

(a)

(b)

图 2-1 机构总图(a)solidworks渲染图;(b)实物图

3

电子科技大学学士学位论文

2.1.1 机械结构运动原理
本章所述的机构结构如图2-1(b)所示，其中3508电机提供驱动转矩，曲柄连杆
装置将电机的旋转运动转化为激光雷达底座在航向角(yaw)方向上的往复运动，同
时绝对值磁编码器记录激光雷达在航向角上的角度变化，以供多传感器融合时使
用。

X

Y

Si
deVi
ew

TopVi
ew

图 2-2 激光雷达坐标系

激光雷达自身的坐标系如图2-2所示，为右手系，其坐标原点在激光雷达的体
心。由于激光雷达的传感器性质，导致其在Z轴方向上的点云十分稀疏，因此，本
文希望通过机构让激光雷达绕Y轴（也就是上文所述的航向角方向）做往复旋转
运动，并通过注册多帧激光雷达的点云来实现激光雷达点云在Y轴方向上的稠密
化。
在著名的激光SLAM算法LOAM[1] 中，由于当时的条件限制，其文章作者没
有三维激光雷达来进行SLAM，而是用一个舵机给一个二维激光雷达进行竖直方
向的往复旋转来增加线数，如图2-3所示。
该机构最大的优点就是结构简单，这也是本文初次采用的机构设计。然而该
机构有以下几个缺点：
1.在往复运动中，当运动方向发生改变时，由于舵机控制精度的问题，很难
做到平滑换向，并且经常伴随有较大的震动，给之后的传感器融合算法带
来了困难。

4

第2章 三维感知传感器机构设计

图 2-3 LOAM中的机构设计

2.LOAM中采用的是二位激光雷达，重量较轻，而本文需要带动三维激光雷
达进行往复运动，重量较重（近1kg），长时间使用舵机带动会使舵机产生
较为明显的回程间隙，影响角度的测量与后续的传感器融合的效果。
因为上述原因，我们没有采用这种结构设计，而是采用了之前提到的曲柄连
杆机构来针对三维激光雷达进行往复运动，相较于上述机构，曲柄连杆结构有以
下几个优点：
1.换向平滑。执行电机只需要一直向同一方向旋转，曲柄连杆机构就能够自
动换向，并且输出的角度曲线近似正弦曲线。
2.对执行机构负担小。仅需要较小并且较为恒定的转矩就能够驱动较大的负
载做往复运动。
3.对执行机构的控制要求低。在该机构中，无刷电机只需要输出恒定的转矩
就能够完成三维激光雷达在偏航角方向上的往复运动，并且经过验证，其
角度输出近似正弦曲线，而若采用上述的舵机机构，要想得到相近的角度
曲线，则对舵机的软件控制提出了较高的要求。
综上，本文选择曲柄连杆机构作为该机构的驱动机构。

2.1.2 曲柄连杆机构的设计
该三维感知机构的一个难点在于如何设计与电机相连的曲柄连杆机构。这里
参照《机械设计基础》 [2] 一书中的相应章节对曲柄四连杆机构的连杆长度进行求
解。
如图2-4所示，假设已知该铰链四杆机构两连架杆𝐴𝐵和𝐶𝐷所形成的角度𝜓1 和𝜑1 在
三个不同位置下的角度，要求连杆a、b、c、d的尺寸。则根据向量向x、y轴投影，
有

5

电子科技大学学士学位论文

图 2-4 四杆机构的数学模型

𝑎 cos 𝜑 + 𝑏 cos 𝛿 = 𝑑 + cos 𝜑
𝑎 sin 𝜑 + 𝑏 sin 𝛿 = 𝑐 sin 𝜑
将上两式先进行移项，然后作平方和相加，从中消去𝛿后整理有

𝑏2 = 𝑎2 + 𝑐2 + 𝑑2 + 2𝑐𝑑 cos 𝜓 − 2𝑎𝑑 cos 𝜑 − 2𝑎𝑐 cos(𝜑 − 𝜓)
我们设
⎧
⎪
⎪
𝑅1 = (𝑎2 + 𝑑2 + 𝑐2 − 𝑏2 )
⎪
⎪
⎨
𝑅2 = 𝑑/𝑐
⎪
⎪
⎪
⎪
⎩𝑅3 = 𝑑/𝑎
代入，则上一个式子可以化简为
𝑅1 − 𝑅2 cos 𝜑 + 𝑅3 cos 𝜓 = 𝑐𝑜𝑠(𝜑 − 𝜓)
这个式子即为铰链四连杆机构的角位置方程， 该方程有三个待定参数𝑅1 、 𝑅2 、
𝑅3 。故应有三组对应的𝜓1 和𝜑1 角才能得出这个方程的解。将三组𝜓1 和𝜑1 角代入
求解该方程后，可以得到四个构件之间的长度关系为
⎧
⎪
⎪
𝑎 = 𝑑/𝑅3
⎪
⎪
⎨
𝑐 = 𝑑/𝑅2
⎪
⎪
√︀
⎪
⎪
⎩𝑏 = 𝑎2 + 𝑐2 + 𝑑2 − 2𝑎𝑐𝑅1

6

第2章 三维感知传感器机构设计

则根据机构的具体设置情况，知道𝑎, 𝑏, 𝑐, 𝑑中的任何一条边的长度后，便可知
剩下四条边的长度。
在实际设计中，我们已知𝜓1 和𝜑1 的三组对应角度为
⎧
⎪
⎪
𝜓1 = 30 ∘ 𝜑1 = 36.3∘
⎪
⎪
⎨
𝜓1 = 60 ∘ 𝜑1 = 43.87∘
⎪
⎪
⎪
⎪
⎩𝜓1 = 120 ∘ 𝜑1 = 35.75∘
并且根据我们的机构设置，构件𝑑的长度为105.72mm。将这些已知量代入公式中
可得
⎧
⎪
⎪
𝑎 = 31.6𝑚𝑚
⎪
⎪
⎨
𝑏 = 49.18𝑚𝑚
⎪
⎪
⎪
⎪
⎩𝑐 = 108.37𝑚𝑚
由此，便得到了曲柄机构的连杆构件设计参数。

2.2 三维感知传感器机构的电路设计

(a)

(b)

(c)

(d)

图 2-5 驱动器与传感器(a)C620 电调;(b)M3508无刷电机;(c)磁编码器;(d)RM A型开发板

2.2.1 驱动器
该三维感知机构采用的驱动器为DJI C620电调，如图2-5(a)所示。该电调支
7

电子科技大学学士学位论文

持50-500Hz的PWM（脉宽调制）信号控制以及CAN总线指令控制，最高支持20A的
持续电流，支持对CAN总线上的电调快速设置ID，支持通过CAN总线获取电机温
度、转子位置和转子速度等信息，切换电机时可无需进行位置传感器的参数校
准。

2.2.2 执行机构
该三维感知机构采用的执行机构为DJI M3508无刷电机，如图2-5(b)所示。该
电机可搭配上文所述C620电调实现正弦驱动，相比传统方波驱动具有更高的效
率、机动性和稳定性。其最高可持续输出力矩为2.8Nm，满足驱动曲柄四连杆机
构的需求。

2.2.3 传感器
2.2.3.1 角度传感器

该三维感知机构采用的角度传感器为傲蓝13线磁编码器，如图2-5(c)所示。该
编码器采用RS485方式通信，其单圈分辨率为8192cpr, 精度为±0.1度。
该编码器为绝对值式编码器，其相对于增量式编码器不同点在于，增量式编
码器以上电时的位置为零点，每次使用都要机械对位；而绝对值式编码器能够记
录机构的唯一位置，即单圈内编码器的每一个示值，都唯一对应了空间中机构的
位置与角度。考虑到我们曲柄连杆机构的特性，显然绝对值式编码器更加符合我
们的要求。
2.2.3.2 激光雷达

该三维感知机构采用的激光雷达为速腾聚创的RS-LiDAR-16，如图2-6所示。
该激光雷达为16线激光雷达，其测距范围为50cm-150m，精度误差为±2𝑐𝑚。垂
直视场角为30度， 其角分辨率为2度； 水平视场角为360度， 其角分辨率为0.090.36度（对应的点云频率为5Hz-20Hz）。

2.2.4 主控板
该三维感知机构采用的主控板为DJI Robomaster A型开发板， 如图2-5(d)所
示。该开发板具备类型丰富的接口，包括12V、5V、3.3V电源接口、CAN接口、
UART接口、 可变电压PWM接口、 SWD接口等。 同时该开发板拥有电源输入的
防反接、过压保护、缓启动、12V电源输出过流保护、PWM端口的ESD等多重保
护。
8

第2章 三维感知传感器机构设计

图 2-6 速腾16线激光雷达

2.2.5 电路拓扑
该三维感知机构的电路拓扑如图2-7所示。激光雷达通过千兆网接口将点云传
输到mini PC上，磁编码器通过485转USB与mini PC通信，同时主控通过PWM控制
电调输出，调节M3508电机的转速，M3508电机提供曲柄四连杆机构的驱动力矩，
而磁编码器又将曲柄机构作用在激光雷达底座上的旋转通过485通信输出到mini
PC。

2.3 三维感知传感器机构的软件设计与运动控制
根据上文所述的机械设计以及电路设计，该三维感知机构的软件设计主要实
现了以下几个任务：
1.实现了各个传感器、 主控到Mini　PC的通信， 同时将数据以ROS （Robot
Operating System）话题的方式发布出去，以供第二章节提到的多帧融合算
法使用。
2.实现了电机的多档调速功能。为了应对不同的场景，在主控中实现了多档
调速功能，以调节曲柄机构的往复运动频率。
3.实时检测电调的温度信息，提供了基于温度检测的堵转保护（温度过高自
动切断控制）。
此外，本文还记录了在电机输出恒定转速情况下的曲柄连杆机构的输出的角
度信息，如图2-8所示。该图纵坐标为角度制的输出角度。从图中可以看出，本章
9

电子科技大学学士学位论文

图 2-7 电路拓扑

10

第2章 三维感知传感器机构设计

所设计的三维感知机构其输出角度近似正弦曲线，并且没有较大的换向震动，相
比起上文所提到的舵机的结构拥有稳定可靠的优势。

图 2-8 曲柄连杆机构输出角度

2.4 本章小结
本章提出了一种新的三维感知机构，并从该机构的机械设计、电路设计以及
软件设计和运动控制三个方面介绍了该机构。在机械方面，本文提出使用无刷电
机+曲柄摇杆机构来代替简单的舵机给三维激光雷达提供一个竖直航向角方向上
的往复运动，这种结构的优势在于机构换向流畅、控制简单以及对机构负载小，
能够为本文后续章节提到的激光雷达的多帧融合提供结构上的稳定与可靠性。在
电路方面，本章利用绝对值式磁编码器对曲柄机构运动的角度进行了记录与输
出，相较于增量式编码器，磁编码器不需要保证每次上电时机构都在同一个位置，
为机械结构的设计提供了便利。在软件方面，本章实现了各个传感器与主控以
及mini PC的通信，主控对无刷电机的多档控制以及对电机的堵转保护，并且绘制
了输出角度，验证了机构的可行性。本文的后续章节将利用该机构输出的点云信
息以及角度信息来进行点云的多帧融合稠密化，并且在融合的点云上进行三维障
碍物的检测与分类。

11

电子科技大学学士学位论文

第3章 点云的多帧融合与激光雷达和相机标定

根据本文第二章所提到的机构，能够将三维激光雷达在其yaw角方向上提供
一个有规律的正弦往复运动。本文提出该机构的主要目的为将激光雷达在时间轴
上的多帧点云进行融合，进而增加激光雷达在竖直方向的分辨率，达到近似于给
激光雷达增加线数的效果。本章将对上述机构得到多帧激光雷达点云进行融合，
并且在目前机构的基础上进行激光雷达与相机的标定，从而使得融合后的点云能
够应用于第四章提到的基于视觉与激光融合的三维障碍物检测方法。

3.1 激光雷达点云的多帧融合
3.1.1 一种朴素的多帧融合策略
点云的注册（registration）是指将有重合部分的点云进行对齐的一项技术。其
关键核心为求出给定点云的坐标系相对于目标点云所在坐标系的旋转与平移，借
以将给定点云转换到目标点云坐标系中，丰富目标点云的信息，以给之后的点云
分割与分类任务提供便利。

图 3-1 三维感知机构的旋转

12

第3章 点云的多帧融合与激光雷达和相机标定

如图3-1所示，本文认为当激光雷达航向角与地面平行时，其激光雷达坐标
系为参照系𝑓 ，第二章所述的三维感知机构目的就在于在激光雷达做往复运动时，
将激光雷达每帧点云在参照系𝑓 上进行注册，最后在曲柄机构的一个运动周期后
将点云融合输出。当机构运动时，激光雷达自身坐标相对于𝑓 发生了旋转，因而
在激光雷达坐标系中的点云相对于𝑓 有一个𝛼角度的旋转量。因而要将此时的点云
注册到𝑓 坐标系中时，要抵消因机构旋转而造成的坐标系的旋转。
最为直观的策略就是，读取绝对值编码器返回的角度𝛼，将激光雷达每帧点
云沿着航向角方向旋转−𝛼的角度，然后注册多帧的激光雷达点云并发布。
在实际的实现过程中，编码器返回角度的频率约为30Hz，而点云发布的频率
约为10Hz，在将点云旋转−𝛼角度时，对𝛼角进行了线性插值以便获得更加精确的
结果。同时，根据计算曲柄机构的角度是增加还是减少，来判断曲柄机构的运动
方向，并且将曲柄机构运动角度为一个正弦周期内的点云融合为一帧新的点云输
出。
然而这种朴素的融合策略在实际中效果不好，体现在融合后的点云所显示的
物体轮廓失真严重，如图3-4(a)所示，原因是没有考虑激光雷达的运动对激光雷达
点云生成的影响。

3.1.2 点云的运动畸变的形成与矫正
在激光雷达点云的多帧融合中，如果只是进行简单的历史点云叠加（如上文
所示），那么融合后的点云相较于真实情况会有很严重的失真，其原因就在于第
二章所提到的三维感知机构在给激光雷达在偏航角方向上的往复运动时，点云会
产生不可忽视的运动畸变。本章节首先介绍什么是激光雷达点云的运动畸变，然
后提出一种通过插值的方式矫正激光雷达的运动畸变。
3.1.2.1 点云的运动畸变

激光雷达的点云的形成本质上是由激光雷达内部的多个激光测距器将一个旋
转周期内的各个测量值记录下来并同时发布后得到的。因此点云中的每个点并不
是在同一时刻被测量出来的。如果激光雷达在测量的过程中也在运动，那么激光
雷达的点云可能会发生畸变[3] 。
下面以二维激光雷达为例，介绍激光雷达点云运动畸变的形成。
图3-2(a)中的黑色的线条表示二维激光雷达处在的真实环境的轮廓图，箭头表
示二维激光雷达的运动方向。图3-2(b)中的蓝色的线条表示二维激光雷达的原始
13

电子科技大学学士学位论文

(a)

(b)

图 3-2 激光雷达运动畸变(a)ground truth;(b)采集得到的数据

数据。注意到其已经发生了畸变，因为二维激光雷达内的激光测距器通过逆时针
的方向旋转，使得右上角的数据优先得到，而左上角的数据在激光雷达向箭头方
向运动了一段距离之后才进行测量，自然导致了运动畸变的产生。
值得一提的是上图表示的二维激光雷达发生的运动畸变是当激光雷达运动方
向为水平运动方向时造成的，而当激光雷达在空间中有垂直方向的旋转运动时，
其造成的运动畸变远比水平运动严重。这是因为激光雷达旋转一周的时间普遍
在0.1秒左右，其水平运动的距离往往很小可以忽略不计。而当激光雷达有竖直方
向的旋转时，即使在0.1s内只有2度的航向角的旋转（这在本文的机构中并不算很
快），在测量20m处的物体时，其运动造成的点云畸变可使得点云的同一线上的第
一个点与最后一个点的垂直相差将近70cm。
综上所述，对于第二章所述的机构，由于其施加了在激光雷达航向角方向上
的旋转，因此导致其在竖直方向上的运动畸变不可忽视，从而简单的叠加点云会
导致在做激光雷达的物体检测时的失真。
3.1.2.2 运动畸变的矫正

对于本文所提到的三维感知机构在航向角方向上的旋转所产生的运动畸变的
矫正，一个较为朴素的方法是，依次遍历激光雷达每帧点云中的每个点，计算其
产生的时间戳𝑡，对磁编码器的角度进行插值，计算出在时间戳𝑡上的角度𝛼，然后
将该点绕原点在航向角方向旋转−𝛼的角度。这个方法最为直观，然而激光雷达每
帧点云高达数十万个点，如果对每个测量得到的点进行插值，则在一秒内要进行
近百万次的插值与旋转操作，显然对于无人驾驶汽车上的移动处理器平台来说这
是不现实的。
14

第3章 点云的多帧融合与激光雷达和相机标定

因此本文提出一个假设，设点云中第一个点产生的时间戳为𝑡0 ，最后一个点
产生的时间戳为𝑡𝑘 ， 则将𝑡0 到𝑡𝑘 之间的时间均匀分为𝑛份， 每份长为Δ𝑡。 本文假
设𝑡0 至𝑡0 +Δ𝑡、𝑡0 +Δ𝑡至𝑡0 +2Δ𝑡...𝑡𝑘 −Δ𝑡至𝑡𝑘 这些时间段，每个时间段内的激光雷
达的测距点的产生的时间都是相同的，为其第一个点的产生时间。根据这个假设，
每个时间段内的所有点都只要进行相同角度变换即可进行运动畸变的矫正。遵循
该假设，则每帧点云只需要进行n次角度插值即可，极大地减小了运算量。同时
虽然该假设认为同一时间段内的点云是同一时间产生的，每个时间段内的点仍有
运动畸变的影响，然而在实验过程中发现，只要当𝑛取一个不太小的值（𝑛 ≥ 50），
则该假设的所产生的时间段内的运动畸变产生的影响很小，可忽略不计。
在实际的程序实现中，由于激光雷达每帧点云是分段传输的，如图3-3所示。
RS-LiDAR-16激光雷达采用UDP协议向PC传输点云信息，而UDP协议相较于TCP协

图 3-3 点云的传输

议，发送数据之前不需要双方建立链接，并且发送的数据没有校验，也没有丢包
的检测，因此不适合一次性发送大量数据给PC。该激光雷达将一帧点云分为84个
UDP包(UDP Packet)，每个包中的点云都是激光雷达旋转360/84 = 4.28度后得到
的16线的点云的集合。当激光雷达的驱动程序接收到84个UDP Packet之后，将这84
个Packet合并成一帧点云输出。
本文修改了RS-LiDAR-16的ROS 驱动程序，将每个UDP Packet不经过合并直
15

电子科技大学学士学位论文

接发布出去，同时在多帧融合的程序里，对每个Packet（而不是每帧）分别进行
编码器角度的插值与点云的旋转，最后再将旋转后的84个Packet进行合并发布。

3.1.3 矫正运动畸变后的多帧融合策略
如上文所言，对单帧点云进行多次插值之后的多帧融合算法流程如Algorithm
1所示；
。
在Algorithm 1中，FindeClosestAngles函数查找在所有的编码器角度中，时间
上离packet的时间戳最近的两帧编码器的角度值，随后对这两个角度进行线性插
值得到packet时间戳下的机构的角度。RotatePointCloudByYaw函数将特定的点云
绕激光雷达坐标系原点旋转指定的角度。最后将每个矫正后的packet合并到一个
新的矫正后的点云中并发布出去即得到了矫正因机构而产生的运动畸变后的点
云。
在图3-4(b)中展示了消除运动畸变后的激光雷达测得的轿车的点云图案。相
较于图3-4(a)，其点云图案没有出现明显的失真，并且通过机构进行多帧融合后的
点云，能够明显的看出轿车的轮廓细节信息，包括车的反光镜、前挡风玻璃等。
而矫正畸变前的轿车点云则很难分辨出这些细节，并且由于运动畸变的作用，其
体积明显比矫正后的点云更大一些。由此证明了本文提出的多帧融合算法拥有较
好的矫正畸变的效果。

(a)

(b)

图 3-4 多帧融合后的轿车点云(a)矫正畸变前;(b)矫正畸变后

16

第3章 点云的多帧融合与激光雷达和相机标定

Algorithm 1 Improved multiple pointcloud fusion algorithm
1: for packet ∈ point cloud do
2:

𝛼1 , 𝛼2 = FindeClosestAngles(encorderAngles, packet.timestamp) 　　

3:

1 .𝑡𝑖𝑚𝑒𝑠𝑡𝑎𝑚𝑝
𝛼 = 𝛼1 + (𝛼2 − 𝛼1 ) × 𝑝𝑎𝑐𝑘𝑒𝑡.𝑡𝑖𝑚𝑒𝑠𝑡𝑎𝑚𝑝−𝛼
𝛼2 .𝑡𝑖𝑚𝑒𝑠𝑡𝑎𝑚𝑝−𝛼1 .𝑡𝑖𝑚𝑒𝑠𝑡𝑎𝑚𝑝

4:

rectifiedPacket = RotatePointCloudByYaw(packet, −𝛼)

5:

rectifiedPointcloud += rectifiedPacket

6:

return rectifiedPointcloud

3.2 激光雷达与相机的标定
自动驾驶无人车是一个多传感器的系统，多传感器信息的融合可以使整个无
人车系统的决策更加智能。激光雷达虽然能够获取较为精确的点云信息，然而点
云信息只包含了三维距离信息。而相机可以通过图像获得大量信息诸如颜色、纹
理信息等，但是其受光照与天气条件影响严重，并且从单目图像中无法获取三维
结构信息。为了同时收集三维信息与物体的颜色与纹理信息，激光雷达与相机经
常进行传感器的数据融合，来为多传感器系统提供更稳定的数据支持。为了进行
数据的融合，首先得知道相机坐标系与激光雷达坐标系之间的旋转与平移关系，
因此，激光雷达与相机的标定就显得尤为重要了。
本章节后续将介绍一种文献[4] 提到的,利用两张贴有ArUco Marker[5] 的标定板
所提供的3d-3d特征匹配的方法，来进行相机与激光雷达的标定。

3.2.1 标定板

图 3-5 标定环境

17

电子科技大学学士学位论文

本文参照文献[4] 提到的方法，制作了两块长约40cm，宽约27cm的长方形硬纸
板材质的标定板，并且将两块ArUco Marker粘在硬纸板的固定位置上，如图3-5所
示。虽然一块标定板已经可以得到四组3d-3d匹配点来解决标定问题，本文仍然采
用了两块标定板，目的是构造多于四对的匹配点来减小标定误差。

3.2.2 相机坐标系中的三维特征点提取
ArUco markers是一种经过特定编码的二维码图案，用以实现对二维码自身
的定位与畸变矫正。 更多细节可以参考文献[5] 。 该文献提出， 通过特定的机器
视觉算法检测到marker的四个角点后， 可以对marker上的二维码进行解码运算，
进而求得二维码的id与四个角点的顺序。而通过输入marker的边长后，还能够通
过PnP[6] 求解出相机坐标系到marker自身坐标系的转换。

图 3-6 ArUco markers

在本文中，将ArUco marker粘在硬纸板的矩形标定板上，并且测量得出硬纸
板的四条边长以及marker在硬纸板中的位置，即可得到硬纸板的四个角点在marker
坐标系中的位置。而本文通过ROS中aruco ros以及aruco mapping[7] 两个程序包可
以检测ArUco marker的位置，进而得到相机坐标系到marker坐标系的转换，从而
得到相机坐标系下的硬纸板的四个角点的位置。相机坐标系下角点位置的计算公
式为
⎡
⎤
𝑥𝑐𝑎𝑚𝑒𝑟𝑎
⎢
⎥
⎢𝑦
⎥
⎢ 𝑐𝑎𝑚𝑒𝑟𝑎 ⎥
⎢
⎥
⎢ 𝑧𝑐𝑎𝑚𝑒𝑟𝑎 ⎥
⎣
⎦
1

⎡

⎤

𝑅11 𝑅12 𝑅13 𝑡𝑥
⎥
⎢
⎥
⎢𝑅
⎢ 21 𝑅22 𝑅23 𝑡𝑦 ⎥
=⎢
⎥
⎢𝑅31 𝑅32 𝑅33 𝑡𝑧 ⎥
⎦
⎣
0
0
0
1

⎤

⎡

𝑥𝑎𝑟𝑢𝑐𝑜
⎢
⎥
⎢𝑦
⎥
⎢ 𝑎𝑟𝑢𝑐𝑜 ⎥
⎥
⎢
⎢ 𝑧𝑎𝑟𝑢𝑐𝑜 ⎥
⎣
⎦

(3-1)

1

其中，𝑥𝑐 𝑎𝑚𝑒𝑟𝑎是相机坐标系中角点的坐标，𝑥𝑎 𝑟𝑢𝑐𝑜是ArUco marker坐标系中
的角点坐标，而上式中的[𝑅|𝑡]矩阵则是相机坐标系相对于marker坐标系的欧式变
换矩阵。
18

第3章 点云的多帧融合与激光雷达和相机标定

3.2.3 LiDAR坐标系中的三维特征点提取
本文所参照的标定方法，其在LiDAR坐标系中的三维特征点提取是通过直线
拟合的方法进行的。如图3-7所示。图中显示的点为激光雷达的点云投影到相机图
像上之后进行边缘检测后所形成的点。在得到该幅图像后，需要手动框选标定板
的每条边上的点，随后标定程序会对这些点进行直线拟合，每两条直线的交点即
为所求的LiDAR坐标系中的三维点。

图 3-7 直线拟合提取特征点

3.2.4 刚体变换求解
在得到两个坐标系下的三维特征匹配点之后，两个坐标系之间的[𝑅|𝑡]刚体变
换矩阵可以通过使用迭代最近点 （Iterative Closest Point, ICP） [8] 算法求得。 假
设𝑃 ，𝑄为R3 中的一组对应点，ICP算法尝试使经过刚体变换后的P点集与Q点集
的重合误差最小。其求解问题可以表述为式3-2所示。
arg min

‖(𝑅𝑃 + 𝑡) − 𝑄‖2

𝑅∈𝑆𝑂(3),𝑡∈R3

19

(3-2)

电子科技大学学士学位论文

一般来说，ICP问题认为对于点集𝑃 中的每个点，在点集𝑄中与之对应的点为其距
离最近的点，依次确认匹配关系后，该算法通过减小两个点集的欧氏距离来对齐
两个点集。
ICP的方法找到两组点的匹配关系，在没有良好的初始值的情况下，极易发
生误匹配，导致算法最后没办法收敛到一个正确的解。考虑到在本文的标定环
境下，两组点的对应关系的是已知的，则两组点之间的[𝑅|𝑡]即存在一个闭式解。
Kabsch算法[9][10] 提供了找到两组对应点间的旋转矩阵的闭式解的方法，而两组点
间的平移量可以在进行旋转对齐后求得。下面参照文献[10] ，介绍Kabsch算法的主
要步骤。
首先假设旋转已知，求两个点集𝑃 与𝑄之间的平移量，设
𝑛
∑︁
𝐹 (𝑡) =
‖(𝑅𝑃𝑖 + 𝑡) − 𝑄𝑖 ‖2

(3-3)

𝑖=1

对上式进行求导，令两边等于0，得
𝑛
∑︁
𝜕𝐹 (𝑡)
=2
((𝑅𝑃𝑖 + 𝑡) − 𝑄𝑖 ) = 0
𝜕𝑡

(3-4)

𝑖=1

因为
𝑛

𝑛

∑︁
∑︁
𝜕𝐹 (𝑡)
= 2𝑅
𝑃𝑖 + 2𝑛𝑡 − 2
𝑄𝑖
𝜕𝑡
𝑖=1

(3-5)

𝑖=1

可得
𝑛

𝑛

1 ∑︁
1 ∑︁
𝑡=
𝑄𝑖 − 𝑅
𝑃𝑖
𝑛
𝑛

(3-6)

𝑡 = 𝑄 − 𝑅𝑃

(3-7)

𝑖=1

𝑖=1

将式3-7的结果代入式3-3中，有
𝑅 = arg min

𝑛
∑︁

‖(𝑅(𝑃𝑖 − 𝑃 ) − (𝑄𝑖 − 𝑄))‖2

(3-8)

𝑅∈𝑆𝑂(3) 𝑖=1

令
𝑋 = 𝑃𝑖 − 𝑃 , 𝑋 ′ = 𝑅𝑋, 𝑌 = 𝑄𝑖 − 𝑄
则目标函数可化简为
𝑛
∑︁

2

‖𝑋𝑖′ − 𝑌𝑖 ‖ = 𝑇 𝑟((𝑋 ′ − 𝑌 )𝑇 (𝑋 ′ − 𝑌 ))

𝑖=1

20

(3-9)

第3章 点云的多帧融合与激光雷达和相机标定

使用矩阵的迹的性质，上式可化简为
𝑇

𝑇 𝑟((𝑋 ′ − 𝑌 )T (𝑋 ′ − 𝑌 )) = 𝑇 𝑟(𝑋 ′ 𝑋 ′ ) + 𝑇 𝑟(𝑌 𝑇 𝑌 ) − 2𝑇 𝑟(𝑌 𝑇 𝑋)

(3-10)

考虑到𝑅是一个旋转矩阵，旋转矩阵必定正交且行列式为1，也就是说，‖𝑋 ′ 𝑖 ‖2 =
‖𝑋𝑖 ‖2 ，则有
′

T

′

𝑇 𝑟((𝑋 − 𝑌 ) (𝑋 − 𝑌 )) =

𝑛
∑︁

(|𝑋𝑖 |2 + |𝑌 𝑖 |2 ) − 2𝑇 𝑟(𝑌 𝑇 𝑋)

(3-11)

𝑖=1

∑︀𝑛

可知

𝑖=1 (|𝑋𝑖 |

2 + |𝑌

2
𝑖 | )项与旋转矩阵𝑅无关，将其从目标函数中消去，有

𝑅 = arg min 𝑇 𝑟(𝑌 𝑇 𝑋 ′ )

(3-12)

𝑅∈𝑆𝑂(3)

将𝑋 ′ = 𝑅𝑋代入，并利用矩阵迹的性质，有
𝑇 𝑟(𝑌 𝑇 𝑋 ′ ) = 𝑇 𝑟(𝑌 𝑇 𝑅𝑋) = 𝑇 𝑟(𝑋𝑌 𝑇 𝑅)

(3-13)

对𝑋𝑌 𝑇 进行SVD分解，有𝑋𝑌 𝑇 = 𝑈 𝐷𝑉 𝑇 ，则
𝑇

𝑇

𝑇

𝑇 𝑟(𝑋𝑌 𝑅) = 𝑇 𝑟(𝑈 𝐷𝑉 𝑅) = 𝑇 𝑟(𝐷𝑉 𝑅𝑈 ) =

3
∑︁

𝑑𝑖 𝑣𝑖𝑇 𝑅𝑢𝑖

(3-14)

𝑖

令𝑀 = 𝑉 𝑇 𝑅𝑈 ，则𝑀 由正交矩阵相乘而得，故易知其亦为正交矩阵，并且𝑑𝑒𝑡(𝑀 ) =
±1。因此M的每个列向量的模均为1，列向量的每个元素均小于等于1。则有
𝑇

𝑇 𝑟(𝑋𝑌 𝑅) =

3
∑︁
𝑖

𝑑𝑖 𝑀𝑖𝑖 ≤

3
∑︁

𝑑𝑖

(3-15)

𝑖

令上式值最大，则得到𝑀𝑖𝑖 = 1，因此𝑀 = 𝐼，为单位矩阵。有
𝑀 = 𝐼 ⇒ 𝑉 𝑇 𝑅𝑈 = 𝐼 ⇒ 𝑅 = 𝑉 𝑈 𝑇

(3-16)

需要注意的是，𝑅应当是一个旋转矩阵，也就是说，𝑅 ∈ 𝑆𝑂(3)，所以应当确保
𝑑𝑒𝑡(𝑅) = +1。如果由上式得到的𝑅，其特征值为-1，则其为不满足条件的解，需
要找到使𝑇 𝑟(𝑌 𝑇 𝑋 ′ )第二大的R，即
𝑇 𝑟(𝑌 𝑇 𝑋 ′ ) = 𝑑1 𝑀11 + 𝑑2 𝑀22 + 𝑑3 𝑀33 𝑤ℎ𝑒𝑟𝑒 𝑑1 ≥ 𝑑2 ≥ 𝑑3 𝑎𝑛𝑑 |𝑀𝑖𝑖 | ≤ 1 (3-17)
在上式中，当𝑀11 = 𝑀22 = 1且𝑀33 = −1时，该式值第二大。将上述情况考虑进
去，则𝑅的闭式解为𝑅 = 𝑈 𝐶𝑉 𝑇 ，其中C为一个矫正矩阵，
⎡
⎤
1 0
0
⎢
⎥
⎥
𝐶 =⎢
0
1
0
⎣
⎦
𝑇
0 0 𝑠𝑖𝑔𝑛(𝑑𝑒𝑡(𝑈 𝑉 ))
21

电子科技大学学士学位论文

3.2.5 多帧刚体变换结合
在实验中发现，由相机图像求得的三维特征点比较稳定，而由于激光雷达本
身存在传感器误差，其通过直线拟合求交点得到的三维角点不够稳定，在相机与
激光雷达位置均固定的情况下，每次测得的激光雷达测得的三维特征角点在会有
一个较小的位移，而这样的位移会对用Kabsch算法得到的解产生较大的误差。
为了减小因激光雷达传感器误差而造成的三维特征角点不准确的问题， 本
文在相机与激光雷达位置固定的情况下，针对多帧图像与点云进行了匹配运算，
对𝑁 次结果进行求取平均值。对平移量的平均值求取公式为
𝑁
1 ∑︁
𝑡=
𝑡𝑣𝑒𝑐𝑖
𝑁
𝑖=1

而对于旋转量的求取平均值，先将上文求得的旋转转为四元数𝑟𝑣𝑒𝑐𝑖 形式，再对四
元数求取平均值。
𝑁
1 ∑︁
𝑟=
𝑟𝑣𝑒𝑐𝑖
𝑁
𝑖=1

𝑟=

𝑟
‖𝑟‖

通过对多帧激光雷达点云与相机进行三维点匹配得到的刚体变换求取平均
值，能够有效的抑制因激光雷达的传感器误差而造成的标定误差。

3.3 本章小结
本章介绍了什么是激光雷达点云注册与运动畸变， 并且提出了插值矫正运
动畸变的方法：在对点云进行多帧融合时，利用点云传输的特性，将点云在时
间上为84个Packet单独发布， 同时线性插值得到这些Packet的时间戳上的编码器
角度值，将这些Packet点云依据编码器的角度注册到参考坐标系中，并且将所有
的Packet点云进行叠加注册后发布为新的点云。由于对每帧点云多进行了84次旋
转角度的插值，从点云图案上看，本文提到的方法极大地改善了运动畸变对点
云注册的影响，完成了矫正运动畸变的目的。同时，本章还介绍了一种利用特
制的标定板在相机与激光雷达坐标系中寻找三维特征匹配点的方法来进行相机的
标定，并详细介绍了如何利用三维匹配点来计算出两个点集所在的坐标系之间的
刚体变换（Kabsch算法）。在后续的章节，本文将利用相机与激光雷达标定出来

22

第3章 点云的多帧融合与激光雷达和相机标定

的[𝑅|𝑡]矩阵，对激光雷达与相机图像的数据进行融合处理，借以进行三维障碍物
的分割与分类。

23

电子科技大学学士学位论文

第4章 基于视觉与三维点云融合的三维障碍物检测方法

随着深度学习领域内的卷积神经网络在目标检测任务中大放异彩，基于视觉
的目标检测的深度学习算法大行其道。在著名的大规模目标检测挑战赛ILSVRC[11]
中， 基于深度学习的目标检测算法连续六年 （2012-2017） 取得了优越的表现。
2017年，ILSVRC的夺冠深度学习网络SENet在目标检测问题上的错误率仅为2.251%，
亦同时宣告了基于图像的目标检测任务基本被攻克。
然而，基于视觉的目标检测无法得到物体的三维位置信息，并且受光照影响
严重。为了解决这些问题，本章后续部分将提出一种激光雷达与相机的多模态传
感器融合技术，其能够在基于视觉的目标检测算法检测到目标时，能够根据激光
雷达的点云得到障碍物的三维位置信息，同时根据激光雷达的点云信息反馈于目
标检测任务的识别上，提升视觉在光照条件不够良好的情况下的检测准确率。

4.1 YOLO-一种实时目标检测网络
在介绍相机图像与激光雷达点云的融合之前，本文将先介绍本文使用的视觉
目标检测算法。本文使用YOLO(You Only Look Once)[12] 算法作为视觉图像上的目
标检测算法。

图 4-1 YOLO-一种实时目标检测网络

24

第4章 基于视觉与三维点云融合的三维障碍物检测方法

早期的目标检测算法通过提取图像的一些特征（例如Haar、SIFT、HOG等），
运用DPM（Deformable Parts Model）模型，并且通过滑动窗口来预测具有较高得
分的区域的策略来进行目标检测。这种传统的方法不仅相当耗时，而且检测准确
率也不是很高。
随后出现了基于object proposal策略的方法， 相比于滑动窗口这种类似于穷
举的方法，该方法大大减少了运算量，同时定位精度也得到了很大的提高。结
合13年兴起的卷积神经网络后，Object detection的性能得到了质的飞跃。
然而，诸如R-CNN、Faster R-CNN这样的网络，因为候选区域较多、网络运
算量较大，因而很难在GPU性能不够强劲的移动机器人场景中做到实时检测。而
本文采用的YOLO网络将目标检测问题转化为一个回归问题。给定图像𝐼，YOLO
网络直接在图像的多个位置上回归出识别目标的边界框（bounding box）以及其类
别。
YOLO没有选择滑动窗口或者提取proposal的策略来训练网络，而是直接将整
张图作为网络的输入，既极大地提升了运算速度，亦很好的区分了图像的前景与
背景区域，而使用proposal策略的Fast R-CNN则经常将背景区域误识别为目标区
域。

图 4-2 YOLO网络结构

图4-2为YOLO的网络结构，从图中可以看出，该目标检测网络拥有24层卷积
层，随后用两层全连接层对结果进行回归输出。
由于其简介的网络结构设计，使得其在实时性能上表现卓越。YOLO能够在
每秒推断（inference）45帧的情况下仍能够保证和Faster R-CNN同样的准确率。基
25

电子科技大学学士学位论文

于其良好的实时性与较为优秀的目标检测准确率，本文将使用YOLO作为视觉与
三维点云融合的目标检测基准方法，并利用激光雷达的点云信息为其检测结果提
供三维位置信息并提高准确率。

4.2 基于YOLO的视觉、三维点云结合的三维障碍物检测
本章所述的传感器设置如图4-3所示。其中激光雷达坐标系到相机坐标系的刚
体变换已由第二章介绍的标定方法得到。在介绍如何将激光雷达点云投影到相机
图像上之前，本文将先介绍相机的成像原理，这是点云投影的理论基础。

图 4-3 传感器设置

4.2.1 相机成像原理
所谓的相机成像，就是相机将三维空间中的坐标映射到二维图像平面的过程。
这一映射可以有许多数学模型去解释，最简单也最为常用的就是针孔成像模型，
如图4-4所示。 现在对该模型进行建模。设图中𝑂 − 𝑥 − 𝑦 − 𝑧为相机坐标系。通
常认为相机坐标系𝑍轴指向相机的前方，𝑋轴指向右方，𝑌 轴指向下方。设真实空
间中有一三维点𝑃 ，经过相机的小孔𝑂成像后，成像点落在相机的成像平面𝑃 ′ 处，
设𝑃 点的坐标为[𝑋, 𝑌, 𝑍]𝑇 ，𝑃 ′ 点的坐标为[𝑋 ′ , 𝑌 ′ , 𝑍 ′ ]，并且设相机的成像平面到模
型中的针孔的距离为𝑓 （焦距）,则根据三角形的相似关系，有
𝑋
𝑌
𝑍
=− ′ =− ′
𝑓
𝑋
𝑌
26

(4-1)

第4章 基于视觉与三维点云融合的三维障碍物检测方法

图 4-4 相机针孔成像原理

整理得
⎧
𝑋
⎪
⎨ 𝑋′ = 𝑓
𝑍
(4-2)
⎪
⎩ 𝑌′=𝑓𝑌
𝑍
实际上，在相机中最后得到的是一个个的像素，设在相机成像平面上存在一
个像素平面𝑂 − 𝑢 − 𝑣，𝑃 ′ 在像素平面的坐标为[𝑢, 𝑣]𝑇 。像素坐标系通常定义其原
点𝑂′ 在图像的左上角，其𝑢轴与𝑣轴的方向与相机坐标系中的𝑋轴与𝑌 轴相同。像
素坐标系相对于相机坐标系，相差了一个平移与缩放。假设相机坐标系在𝑢轴上
缩放了𝛼倍，在𝑣轴上缩放了𝛽倍，同时像素坐标系相对于相机坐标系的原点平移
了[𝑐𝑥 , 𝑐𝑦 ]𝑇 。则𝑃 ′ 在成像平面上的坐标与其像素坐标[𝑢, 𝑣]𝑇 之间的关系为
⎧
⎨ 𝑢 = 𝛼𝑋 ′ + 𝑐𝑥
⎩ 𝑣 = 𝛽𝑌 ′ + 𝑐
𝑦
将上式代入式4-2中，并设𝛼𝑓 = 𝑓𝑥 ，𝛽𝑓 = 𝑓𝑦 ，则有
⎧
𝑋
⎪
⎨ 𝑢 = 𝑓 𝑥 + 𝑐𝑥
𝑍
⎪
⎩ 𝑣 = 𝑓 𝑦 𝑌 + 𝑐𝑦
𝑍

27

(4-3)

(4-4)

电子科技大学学士学位论文

将上式写为矩阵的形式，则有
⎡
⎤⎡ ⎤
⎡ ⎤
𝑓 𝑥 0 𝑐𝑥 𝑋
𝑢
⎥⎢ ⎥
⎢ ⎥ 1⎢
⎢𝑣 ⎥ = ⎢ 0 𝑓 𝑦 𝑐𝑦 ⎥ ⎢ 𝑌 ⎥ , 1 𝐾𝑃
(4-5)
⎦⎣ ⎦ 𝑍
⎣ ⎦ 𝑧⎣
0 0 1
𝑍
1
⎡
⎤
𝑓 𝑥 0 𝑐𝑥
⎢
⎥
⎥称之为相机的内参，通常，相机的标定就是为了得出相机
其中矩阵⎢
0
𝑓
𝑦
𝑐𝑦
⎣
⎦
0 0 1
的内参矩阵。本文在融合激光与相机之前已通过ROS的标定程序得到了相机的内
参𝐾。

4.2.2 激光雷达点云的投影
上式中，𝑃 为相机坐标系内的一点，所以在根据第二章得到的标定方法得出
激光雷达与相机坐标系的变换矩阵𝑇 后，先将激光雷达点云转换到相机坐标系中，
再通过内参矩阵投影到图像上，即
1
𝐾𝑇 𝑃𝐿𝑖𝐷𝐴𝑅
𝑍
其中𝑃𝐿𝑖𝐷𝐴𝑅 表示激光雷达坐标系中的点云中的三维点。
𝑃𝑢𝑣 =

(4-6)

图 4-5 激光雷达点云的投影

本文利用前文提到的三维感知机构进行了多帧点云的注册融合，并将融合后
的点云投影到了相机的图像上，点云投影效果如图4-5所示。其中投影点的颜色
28

第4章 基于视觉与三维点云融合的三维障碍物检测方法

越深，代表其距离相机的位置越近；颜色越浅，代表离相机的距离越远。在将激
光雷达点云投影到图像之后，便可以利用YOLO的检测结果对点云进行分割与分
类。

4.2.3 点云前景与背景的分割
本文首先利用YOLO对相机图像进行了目标检测的推断。YOLO的检测效果
如图4-6所示，其输出为多个边界框与识别的物体的类别，以及YOLO对推断结果
的置信度（confidence）。图4-6为设置YOLO检测的置信度阈值为0.8时输出的结果
（即不输出置信度低于0.8的检测结果）。

图 4-6 激光雷达点云的投影

将点云按照上文投影到图像中，则点云中的每个点𝑃 ′ 都有一个唯一的像素坐
标[𝑢, 𝑣]𝑇 与其对应。剔除像素坐标不在YOLO检测的边界框内的点，便得到了基
于YOLO检测结果的点云分割与分类结果，如图4-7(a)所示。
从图4-7(a)中可以看出，尽管按照上述结果的确将人与汽车的点云分割了出
来，然而由于YOLO得出的边界框内除了识别的目标，还有一些背景物体，体现

29

电子科技大学学士学位论文

(a)

(b)

图 4-7 点云前景与背景的分割(a)原始点云;(b)Kmeans分类后的结果

在点云中就是，除了表示人与轿车的点云，还有一些人与汽车后的灌木的点云，
其在图像上的投影也在候选框里。这些点云如果不加以去除，会对目标物体的三
维位置的确定造成较大的负面影响。
考虑到目标点云与背景点云转换到相机坐标系后，在Z轴方向上有较大的距
离，本文采用K-means算法[13] ，对投影后在同一边界框内的点云进行聚类分割。
K-means算法的目的是，把𝑛个点划分到𝐾个聚类中，使得每个点都属于离它
最近的均值（称之为聚类中心）对应的聚类，以之作为聚类标准。其算法流程
为：
1.假设分为K类；
2.从输入的数据点集合中随机选择一个点作为第一个聚类中心；
3.对于数据集中的每一个点𝑥，计算其与最近的聚类中心(指已选择的聚类中
心)的距离𝐷(𝑥)；
4.选择一个新的数据点作为新的聚类中心，选择的原则是：𝐷(𝑥)较大的点被
选取为聚类中心的概率较大；
5.重复3和4两个步骤直到𝐾个聚类中心被选出来；
6.利用这𝐾个初始的聚类中心，继续重复3-5的步骤，直到聚类结果不发生变
化或者达到最大迭代次数。
对于图4-7(a)中的每个物体边界框内的点云，假设其可以被分为两类：前景与
背景。设原始三维点集为𝑃3 ，将所有点映射到相机坐标系的Z轴上，成为一个新
的一维点集𝑃1 ，并对𝑃1 进行K-means聚类，其中𝐾 = 2。则聚类得到的两类点中，
30

第4章 基于视觉与三维点云融合的三维障碍物检测方法

中心点Z轴坐标较小的即为前景，也就是YOLO检测的物体的点云。图4-7(b)为Kmeans聚类提取得到的结果。从图中可知，采用K-means算法较好的将前景与背景
分割开来，剔除了无关的背景点，只保留了与YOLO检测到的目标有关的点。

4.2.4 目标三维坐标的计算与检测结果的优化
在得到了与YOLO的检测结果相关的目标物体的点云后，物体的三维坐标可
以由求点集的中心点坐标而得。同时，为了更好的表示三维物体的检测结果，本
文还将检测得到的物体用长方体包围盒去拟合，如图4-9所示。包围盒的顶点由点
集的𝑋𝑚𝑖𝑛 , 𝑌𝑚𝑖𝑛 , 𝑍𝑚𝑖𝑛 以及𝑋𝑚𝑎𝑥 , 𝑌𝑚𝑎𝑥 , 𝑍𝑚𝑎𝑥 决定，其中𝑋𝑚𝑖𝑛 表示点集中𝑋轴坐标
最小的点的𝑋坐标，𝑋𝑚𝑎𝑥 表示点集中𝑋轴坐标最大的点的𝑋坐标。

图 4-8 YOLO在低置信度阈值下的误检测

前文提到， 图4-6中的结果是将YOLO检测结果的置信度阈值设为0.8后的检
测结果。而将YOLO检测结果的阈值设为0.4后，YOLO就出现了许多误检测，如
图4-8所示。由于光照等情况的影响，很多时候YOLO的检测结果置信度都不会高

31

电子科技大学学士学位论文

于0.8，而降低置信度阈值又会造成误检测的结果增多。本文提出了一种在YOLO
低置信度阈值下，融合激光雷达点云位置信息来祛除误检测结果的方法。
从图4-8中可以看出，大部分误检测的区域，其边界框对应的三维空间区域可
能没有点云，或者其三维位置信息与边界框面积不相符合。为了祛除误检测结果，
本文首先假设已知每个待检测的目标物体的最小投影矩形。举例来说，对图4-8来
说，认为汽车在相机坐标系中的最小投影矩形为2.4m×1.8m，即对于轿车而言，
至少有2.4m×1.8m的面积在相机视野范围内，那么，祛除误检测的策略流程可以
表述为：
1.如果YOLO检测得到的边界框内没有点云，则认为发生了误检测。
2.如果YOLO检测得到的边界框内有点云，则计算点云中所有点的平均坐标
𝑋𝑚𝑒𝑎𝑛 , 𝑌𝑚𝑒𝑎𝑛 , 𝑍𝑚𝑒𝑎𝑛 。
3.在相机坐标系中构建长方形平面，其顶点坐标为
(𝑋𝑚𝑒𝑎𝑛 − 𝑋ℎ𝑦𝑝𝑜 , 𝑌𝑚𝑒𝑎𝑛 − 𝑌ℎ𝑦𝑝𝑜 , 𝑍𝑚𝑒𝑎𝑛 )
(𝑋𝑚𝑒𝑎𝑛 + 𝑋ℎ𝑦𝑝𝑜 , 𝑌𝑚𝑒𝑎𝑛 − 𝑌ℎ𝑦𝑝𝑜 , 𝑍𝑚𝑒𝑎𝑛 )

(4-7)

(𝑋𝑚𝑒𝑎𝑛 − 𝑋ℎ𝑦𝑝𝑜 , 𝑌𝑚𝑒𝑎𝑛 + 𝑌ℎ𝑦𝑝𝑜 , 𝑍𝑚𝑒𝑎𝑛 )
(𝑋𝑚𝑒𝑎𝑛 + 𝑋ℎ𝑦𝑝𝑜 , 𝑌𝑚𝑒𝑎𝑛 + 𝑌ℎ𝑦𝑝𝑜 , 𝑍𝑚𝑒𝑎𝑛 )
其中，𝑋ℎ𝑦𝑝𝑜 , 𝑌ℎ𝑦𝑝𝑜 为上文提到的最小投影矩阵的长与宽。
4.将该长方形平面通过内参矩阵投影到相机图像中，得到一个估计的长方形
框选面积𝑆ℎ𝑦𝑝𝑜 。
5.将𝑆ℎ𝑦𝑝𝑜 与YOLO检测出的边界框的面积𝑆𝑦𝑜𝑙𝑜 做比较，认为不满足约束条件
0.5 × 𝑆ℎ𝑦𝑝𝑜 ≤ 𝑆𝑦𝑜𝑙𝑜 ≤ 1.5 × 𝑆ℎ𝑦𝑝𝑜

(4-8)

的检测结果为误检测。
经过该点云图像融合祛除误检测策略后，三维物体检测结果如图4-9所示。其
中误检测的部分用红色矩形标出，而正确的检测部分利用上文提出的长方体包围
盒表示出来。可以看出，通过该策略能够有效的排除YOLO在低置信度阈值下的
误检测，并且利用激光雷达的信息，能够很好的计算出检测的目标的三维位置。

4.3 本章小结
本章介绍了YOLO，一种实时的视觉目标检测方法，该方法最大的优点在于
其实时性，能够在较高的识别准确率下，达到每秒推断45帧的速度。并且之后融
32

第4章 基于视觉与三维点云融合的三维障碍物检测方法

合了相机与激光雷达的信息，对三维障碍物进行了分割与分类。首先将激光雷达
的点云信息投影到图像上，利用YOLO 的推断结果，首先对激光雷达点云做了前
景的提取与分割；随后，根据点云信息，计算得出了检测的目标的三维位置信息，
并计算出物体的三维包围盒；最后，针对YOLO在低置信度阈值下容易出现误检
测的问题，本文利用点云信息来验证YOLO的推断结果，并且甄别与祛除了误检
测的输出。从结果中可以看出，融合视觉与三维点云的信息，能够较好的对三维
物体进行检测与识别，为无人驾驶技术中的环境感知与路径规划提供了更多的信
息。

图 4-9 enhanced YOLO

33

电子科技大学学士学位论文

第5章 实验验证与结果分析

本章将对文中提出的三维感知机构的进行仿真，对融合后的点云的注册融合
效果进行了实验分析，并且融合了Odometry的信息。

5.1 Gazebo下的三维感知机构仿真
Gazebo是一个功能强大的三维物理仿真平台， 具备强大的物理引擎、 高质
量的图形渲染、方便的编程与图形接口，最重要的还有其具备开源免费的特性。
Gazebo支持显示逼真的三维环境，包括光线、纹理、影子等。它还支持传感器数
据的仿真，同时可以仿真传感器噪声。
本文利用Gazebo对三维感知机构的运动以及点云融合进行了仿真， 如图所
示。

5.2 三维感知机构结合里程计信息构建三维地图
前文提出的三维感知机构，其实验场景都是在机构位置静止不变的情况下得
到的。而当机构架设在小车上时，由于小车相对于世界坐标系有一个运动，如果
仍然采用之前的融合策略进行点云的输出，则输出的点云会有小车运动方向上的
畸变。
因此，在运动的小车上进行三维感知时，需要结合里程计的信息，计算出机
构在小车运动方向上的位移，借此消除点云在小车运动方向上的畸变。同时，可
以根据小车自身坐标系到里程计坐标系的变换，利用该三维感知机构构建基于小
车里程计坐标系的三维地图。
本文结合图4-3所示的传感器设置，利用小车的里程计对小车行驶过程中的点
云进行了采集、融合与构建了三维地图。本文利用了ROS的tf树

34

第5章 实验验证与结果分析

5.3 后续工作展望
时域积分方程方法的研究近几年发展迅速，在本文研究工作的基础上，仍有
以下方向值得进一步研究：
……

35

电子科技大学学士学位论文

第6章 全文总结与后续工作展望
6.1 全文总结
本文以时域积分方程方法为研究背景，主要对求解时域积分方程的时间步进
算法以及两层平面波快速算法进行了研究。
……

6.2 后续工作展望
时域积分方程方法的研究近几年发展迅速，在本文研究工作的基础上，仍有
以下方向值得进一步研究：
……

36

参考文献

参考文献

[1] J. Zhang, S. Singh. LOAM: Lidar Odometry and Mapping in Real-time.[J]. 2014, 2:9
[2] 王良才等. 机械设计基础[M]. 北京: 北京大学出版社, 2007, 72–73
[3] S. Hong, H. Ko, J. Kim. VICP: Velocity updating iterative closest point algorithm[C]. 2010,
1893–1898
[4] A. Dhall, K. Chelani, V. Radhakrishnan, et al. LiDAR-camera calibration using 3D-3D point
correspondences[J]. arXiv preprint arXiv:1705.09785, 2017
[5] S. Garrido-Jurado, R. Muñoz-Salinas, F. J. Madrid-Cuevas, et al. Automatic generation and
detection of highly reliable fiducial markers under occlusion[J]. Pattern Recognition, 2014,
47(6):2280–2292
[6] V. Lepetit, F. Moreno-Noguer, P. Fua. EPnP: An Accurate O(n) Solution to the PnP Problem[J].
International Journal of Computer Vision, 2008, 81(2):155
[7] J. Bacik. aruco mapping[EB/OL]. http://wiki.ros.org/aruco_mapping, Dec 16, 2016
[8] Z. Zhang. Iterative point matching for registration of free-form curves and surfaces[J]. International Journal of Computer Vision, 1994, 13(2):119–152
[9] W. Kabsch. A solution for the best rotation to relate two sets of vectors[J]. Acta Crystallographica Section A: Crystal Physics, Diffraction, Theoretical and General Crystallography, 1976,
32(5):922–923
[10] O. Sorkine. Least-squares rigid motion using svd[J]. Technical notes, 2009, 120(3):52
[11] F. Li. Imagenet Large Scale Visual Recognition Challenge (ILSVRC)[EB/OL]. http://www.
image-net.org/challenges/LSVRC/, Dec 16, 2010
[12] J. Redmon, S. Divvala, R. Girshick, et al. You only look once: Unified, real-time object detection[C]. 2016, 779–788
[13] J. MacQueen, et al. Some methods for classification and analysis of multivariate observations[C].
1967, 281–297
[14] T. Foote. tf: The transform library[C]. 2013, 1–6
[15] X. Han, J. Lu, Y. Tai, et al. A real-time LIDAR and vision based pedestrian detection system for
unmanned ground vehicles[C]. 2015, 635–639

37

电子科技大学学士学位论文

致 谢

38

外文资料原文

The Name of the Game
1.1 xxx
1.1.1 xxx
1.1.1.1 xxxx

1.2 xxx
1.2.1 xxx
1.2.1.1 xxxx

English words like ‘technology’ stem from a Greek root beginning with the letters
𝜏 𝜖𝜒 . . . ; and this same Greek word means art as well as technology. Hence the name TEX,
which is an uppercase form of 𝜏 𝜖𝜒.TeX (actually TEX), meaning of 𝜏 𝜖𝜒
Insiders pronounce the 𝜒 of TEX as a Greek chi, not as an ‘x’, so that TEX rhymes
with the word blecchhh. It’s the ‘ch’ sound in Scottish words like loch or German words
like ach ; it’s a Spanish ‘j’ and a Russian ‘kh’. When you say it correctly to your computer,
the terminal may become slightly moist.
The purpose of this pronunciation exercise is to remind you that TEX is primarily
concerned with high-quality technical manuscripts: Its emphasis is on art and technology, as in the underlying Greek word. If you merely want to produce a passably good
document—something acceptable and basically readable but not really beautiful—a simpler system will usually suffice. With TEX the goal is to produce the finest quality; this
requires more attention to detail, but you will not find it much harder to go the extra
distance, and you’ll be able to take special pride in the finished product.
On the other hand, it’s important to notice another thing about TEX’s name: The
‘E’ is out of kilter. This logo displaced ‘E’ is a reminder that TEX is about typesetting,
and it distinguishes TEX from other system names. In fact, TEX (pronounced tecks) is
the admirable Text EXecutive processor developed by Honeywell Information Systems.
39

电子科技大学学士学位论文

Since these two system names are Bemer, Robert, see TEX, ASCII pronounced quite
differently, they should also be spelled differently. The correct way to refer to TEX in a
computer file, or when using some other medium that doesn’t allow lowering of the ‘E’,
is to type ‘—TeX—’. Then there will be no confusion with similar names, and people
will be primed to pronounce everything properly.

40

外文资料译文

此名有诗意
1.1 xxx
1.1.1 xxx
1.1.1.1 xxxx

1.2 xxx
1.2.1 xxx
1.2.1.1 xxxx

英语单词“technology”来源于以字母𝜏 𝜖𝜒 . . . 开头的希腊词根；并且这个希腊
单词除了 technology的意思外也有art的意思。因此，名称TEX是𝜏 𝜖𝜒的大写格式。
在发音时，TEX的𝜒的发音与希腊的chi一样，而不是“x”，所以TEX与blecchhh
押韵。“ch” 听起来象苏格兰单词中的loch 或者德语单词中的ach；它在西班牙语
中是“j”，在俄语中是“kh”。当你对着计算机正确读出时, 终端屏幕上可能有点
雾。
这个发音练习是提醒你，TEX主要处理的是高质量的专业书稿：它的重点在
艺术和专业方面, 就象希腊单词的含义一样。如果你仅仅想得到一个过得去——可
读下去但不那么漂亮——的文书, 那么简单的系统一般就够用了。使用TEX的目的
是得到最好的质量；这就要在细节上花功夫, 但是你不会认为它难到哪里去，并且
你会为所完成的作品感到特别骄傲。
另一方面重要的是要注意到与TEX名称有关的另一件事: “E”是错位的。这
个偏移 “E” 的标识提醒人们， TEX与排版有关， 并且把TEX从其它系统的名称
区别开来。实际上，TEX(读音为 tecks)是Honeywell Information Systems 的极好
的Text EXecutive处理器。因为这两个系统的名称读音差别很大，所以它们的拼写
也不同。在计算机中表明TEX文件的正确方法，或者当所用的方式无法降低“E”
时，就要写作“TeX”。这样, 就与类似的名称不会产生混淆, 并且为人们可以正确
发音提供了条件。
41

