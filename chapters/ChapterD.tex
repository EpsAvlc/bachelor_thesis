% !Mode:: "TeX:UTF-8"

\chapter{基于视觉与三维点云融合的三维障碍物检测方法}

随着深度学习领域内的卷积神经网络在目标检测任务中广泛运用，基于视觉的目标检测的深度学习算法大行其道。在大规模目标检测挑战赛ILSVRC\citeup{Li2010lsvrc} 中，基于深度学习的目标检测算法连续六年（2012-2017）取得了优越的表现。2017年，ILSVRC的夺冠深度学习网络SENet在目标检测问题上的错误率仅为$2.251 \%$，亦同时宣告了基于图像的目标检测任务基本被攻克。

然而，基于视觉的目标检测无法得到物体的三维位置信息，并且受光照影响严重。为了解决这些问题，本章后续部分将提出一种激光雷达与相机的多模态传感器融合技术，其能够在基于视觉的目标检测算法检测到目标时，能够根据激光雷达的点云得到障碍物的三维位置信息，同时根据激光雷达的点云信息反馈于目标检测任务的识别上，提升视觉在光照条件不够良好的情况下的检测准确率。

\section{YOLO-一种实时目标检测网络}

在介绍相机图像与激光雷达点云的融合之前，本文将先介绍本文使用的视觉目标检测算法。本文使用YOLO(You Only Look Once)\citeup{redmon2016you}算法作为视觉图像上的目标检测算法。

\pic[htbp]{YOLO-一种实时目标检测网络}{width=0.6\textwidth}{yolo}

早期的目标检测算法通过提取图像的一些特征（例如Haar、SIFT、HOG等），运用DPM（Deformable Parts Model）模型，并且通过滑动窗口来预测具有较高得分的区域的策略来进行目标检测。这种传统的方法不仅相当耗时，而且检测准确率也不是很高。

随后出现了基于object proposal策略的方法，相比于滑动窗口这种类似于穷举的方法，该方法大大减少了运算量，同时定位精度也得到了很大的提高。结合13年兴起的卷积神经网络后，Object detection的性能得到了质的飞跃。

然而，诸如R-CNN、Faster R-CNN这样的网络，因为候选区域较多、网络运算量较大，因而很难在GPU性能不够强劲的移动机器人场景中做到实时检测。而本文采用的YOLO网络将目标检测问题转化为一个回归问题。给定图像$I$，YOLO 网络直接在图像的多个位置上回归出识别目标的边界框（bounding box）以及其类别。

YOLO没有选择滑动窗口或者提取proposal的策略来训练网络，而是直接将整张图作为网络的输入，既极大地提升了运算速度，亦很好的区分了图像的前景与背景区域，而使用proposal策略的Fast R-CNN则经常将背景区域误识别为目标区域。

\pic[htbp]{YOLO网络结构}{width=1\textwidth}{yolo_net}

图\ref{yolo_net}为YOLO的网络结构，从图中可以看出，该目标检测网络拥有24层卷积层，随后用两层全连接层对结果进行回归输出。

由于其简介的网络结构设计，使得其在实时性能上表现卓越。YOLO能够在每秒推断（inference）45帧的情况下仍能够保证和Faster R-CNN同样的准确率。基于其良好的实时性与较为优秀的目标检测准确率，本文将使用YOLO作为视觉与三维点云融合的目标检测基准方法，并利用激光雷达的点云信息为其检测结果提供三维位置信息并提高准确率。

\section{基于YOLO的视觉、三维点云结合的三维障碍物检测}


\subsection{点云前景与背景的分割}
本文首先利用YOLO对相机图像进行了目标检测的推断。YOLO的检测效果如图\ref{yolo_detect}所示，其输出为多个边界框与识别的物体的类别，以及YOLO对推断结果的置信度（confidence）。图\ref{yolo_detect}为设置YOLO检测的置信度阈值为0.8时输出的结果（即不输出置信度低于0.8的检测结果）。
\pic[htbp]{YOLO检测结果}{width=0.9\textwidth}{yolo_detect}

将点云按照上文投影到图像中，则点云中的每个点$P'$都有一个唯一的像素坐标$[u,v]^T$与其对应。剔除像素坐标不在YOLO检测的边界框内的点，便得到了基于YOLO检测结果的点云分割与分类结果，如图\ref{before_cluster}所示。
\begin{pics}[htbp]{点云前景与背景的分割}{cluster}
    \addsubpic{原始点云}{width=0.45\textwidth}{before_cluster}
    \addsubpic{Kmeans分类后的结果}{width=0.468\textwidth}{after_cluster}
\end{pics}

从图\ref{before_cluster}中可以看出，尽管按照上述结果的确将人与汽车的点云分割了出来，然而由于YOLO得出的边界框内除了识别的目标，还有一些背景物体，体现在点云中就是，除了表示人与轿车的点云，还有一些人与汽车后的灌木的点云，其在图像上的投影也在候选框里。这些点云如果不加以去除，会对目标物体的三维位置的确定造成较大的不利影响。

考虑到目标点云与背景点云转换到相机坐标系后，在Z轴方向上有较大的距离，本文采用K-means算法\citeup{macqueen1967some}，对投影后在同一边界框内的点云进行聚类分割。

K-means算法的目的是，把$n$个点划分到$K$个聚类中，使得每个点都属于离它最近的均值（称之为聚类中心）对应的聚类，以之作为聚类标准。其算法流程为：

(1) 假设分为K类；

(2) 任意随机出K个点，认为该K个点为聚类的中心点；

(3) 遍历每一个点$x$，计算其与上一步得出的K个点的欧式距离$D(x)$，将该点归为使得$D(x)$最小的聚类中；

(4) 计算每个聚类内所有点的平均位置，认为其为新的聚类中心；

(5) 重复3和4，直到$K$个聚类中心被选出来；

(6) 利用得到的新的K个点，继续重复3-5的步骤，直到每个点的分类结果不变，或者循环达到迭代次数上限。

对于图\ref{before_cluster}中的每个物体边界框内的点云，假设其可以被分为两类：前景与背景。设原始三维点集为$P_3$，将所有点映射到相机坐标系的Z轴上，成为一个新的一维点集$P_1$，并对$P_1$进行K-means聚类，其中$K=2$。则聚类得到的两类点中，中心点Z轴坐标较小的即为前景，也就是YOLO检测的物体的点云。图\ref{after_cluster}为K-means聚类提取得到的结果。从图中可知，采用K-means算法较好的将前景与背景分割开来，剔除了无关的背景点，只保留了与YOLO检测到的目标有关的点。

\subsection{目标三维坐标的计算与检测结果的优化}

在得到了与YOLO的检测结果相关的目标物体的点云后，物体的三维坐标可以由求点集的中心点坐标而得。同时，为了更好的表示三维物体的检测结果，本文还将检测得到的物体用长方体包围盒去拟合，如图\ref{enhanced_yolo}所示。包围盒的顶点由点集的$X_{min}, Y_{min}, Z_{min}$以及$X_{max}, Y_{max}, Z_{max}$决定，其中$X_{min}$表示点集中$X$轴坐标最小的点的$X$坐标，$X_{max}$表示点集中$X$轴坐标最大的点的$X$坐标。

\pic[htbp]{YOLO在低置信度阈值下的误检测}{width=0.9\textwidth}{yolo_before}

上文提到，图\ref{yolo_detect}中的结果是将YOLO检测结果的置信度阈值设为0.8后的检测结果。而将YOLO检测结果的阈值设为0.4后，YOLO就出现了许多误检测，如图\ref{yolo_before}所示。由于光照等情况的影响，很多时候YOLO的检测结果置信度都不会高于0.8，而降低置信度阈值又会造成误检测的结果增多。本文提出了一种在YOLO 低置信度阈值下，融合激光雷达点云位置信息来去除误检测结果的方法。

从图\ref{yolo_before}中可以看出，大部分误检测的区域，其边界框对应的三维空间区域可能没有点云，或者其三维位置信息与边界框面积不相符合。为了去除误检测结果，本文首先假设已知每个待检测的目标物体的最小投影矩形。举例来说，对图\ref{yolo_before}来说，认为汽车在相机坐标系中的最小投影矩形为2.4m$\times$1.8m，即对于轿车而言，至少有2.4m$\times$1.8m的面积在相机视野范围内，那么，去除误检测的策略流程可以表述为：

(1) 如果YOLO检测得到的边界框内没有点云，则认为发生了误检测。

(2) 如果YOLO检测得到的边界框内有点云，则计算点云中所有点的平均坐标 $X_{mean}, Y_{mean}, Z_{mean}$。

(3) 在相机坐标系中构建长方形平面，其顶点坐标为
\begin{equation}
    \begin{split}
(X_{mean} - X_{hypo}, Y_{mean}-Y_{hypo}, Z_{mean}) \\
(X_{mean} + X_{hypo}, Y_{mean}-Y_{hypo}, Z_{mean}) \\
(X_{mean} - X_{hypo}, Y_{mean}+Y_{hypo}, Z_{mean}) \\
(X_{mean} + X_{hypo}, Y_{mean}+Y_{hypo}, Z_{mean})
    \end{split}
\end{equation}
其中，$X_{hypo}, Y_{hypo}$为上文提到的最小投影矩阵的长与宽。

(4) 将该长方形平面通过内参矩阵投影到相机图像中，得到一个估计的长方形框选面积$S_{hypo}$。

(5)
 将$S_{hypo}$与YOLO检测出的边界框的面积$S_{yolo}$做比较，认为不满足约束条件    
\begin{equation}
    0.5 \times S_{hypo} \leq S_{yolo} \leq 1.5 \times S_{hypo} 
\end{equation}
的检测结果为误检测。


经过该点云图像融合去除误检测策略后，三维物体检测结果如图\ref{enhanced_yolo}所示。其中误检测的部分用红色矩形标出，而正确的检测部分利用上文提出的长方体包围盒表示出来。可以看出，通过该策略能够有效的排除YOLO在低置信度阈值下的误检测，并且利用激光雷达的信息，能够很好的计算出检测的目标的三维位置。

\section{本章小结}

本章介绍了YOLO，一种实时的视觉目标检测方法，该方法最大的优点在于其实时性，能够在较高的识别准确率下，达到每秒推断45帧的速度。并且之后融合了相机与激光雷达的信息，对三维障碍物进行了分割与分类。利用YOLO 的推断结果，首先对激光雷达点云做了前景的提取与分割；随后，根据点云信息，计算得出了检测的目标的三维位置信息，并计算出物体的三维包围盒；最后，针对YOLO在低置信度阈值下容易出现误检测的问题，本文利用点云信息来验证YOLO的推断结果，并且甄别与去除了误检测的输出。从结果中可以看出，融合视觉与三维点云的信息，能够较好的对三维物体进行检测与识别，为无人驾驶技术中的环境感知与路径规划提供了更多的信息。

\pic[htbp]{进行多传感器融合增强后的YOLO检测结果}{width=0.9\textwidth}{enhanced_yolo}