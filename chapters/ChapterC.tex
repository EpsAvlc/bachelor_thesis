% !Mode:: "TeX:UTF-8"


\chapter{点云的多帧融合与激光雷达和相机标定}
根据本文第二章所提到的机构，能够将三维激光雷达在其yaw角方向上提供一个有规律的正弦往复运动。本文提出该机构的主要目的为将激光雷达在时间轴上的多帧点云进行融合，进而增加激光雷达在竖直方向的分辨率，达到近似于给激光雷达增加线数的效果。本章将对上述机构得到多帧激光雷达点云进行融合，并且在目前机构的基础上进行激光雷达与相机的标定，从而使得融合后的点云能够应用于第四章提到的基于视觉与激光融合的三维障碍物检测方法。

\section{激光雷达点云的多帧融合}

\subsection{一种朴素的多帧融合策略}

最为直观的策略就是，读取绝对值编码器返回的角度$\alpha$，将激光雷达每帧点云沿着航向角方向旋转$-\alpha$的角度，然后注册多帧的激光雷达点云并发布。

在实际的实现过程中，编码器返回角度的频率约为30Hz，而点云发布的频率约为10Hz，在将点云旋转$-\alpha$角度时，对$\alpha$角进行了线性插值以便获得更加精确的结果。同时，根据计算曲柄机构的角度是增加还是减少，来判断曲柄机构的运动方向，并且将曲柄机构运动角度为一个正弦周期内的点云融合为一帧新的点云输出。

然而这种朴素的融合策略在实际中效果不好，体现在融合后的点云所显示的物体轮廓失真严重，如图\ref{car_before}所示，原因是没有考虑激光雷达的运动对激光雷达点云生成的影响。

\subsection{点云的运动畸变的形成与矫正}
在激光雷达点云的多帧融合中，如果只是进行简单的历史点云叠加（如上文所示），那么融合后的点云相较于真实情况会有很严重的失真，其原因就在于第二章所提到的三维感知机构在给激光雷达在偏航角方向上的往复运动时，点云会产生不可忽视的运动畸变。本章节首先介绍什么是激光雷达点云的运动畸变，然后提出一种通过插值的方式矫正激光雷达的运动畸变。

\subsubsection{点云的运动畸变}
激光雷达的点云的形成本质上是由激光雷达内部的多个激光测距器将一个旋转周期内的各个测量值记录下来并同时发布后得到的。因此点云中的每个点并不是在同一时刻被测量出来的。如果激光雷达在测量的过程中也在运动，那么激光雷达的点云可能会发生畸变\citeup{hong2010vicp}。

下面以二维激光雷达为例，介绍激光雷达点云运动畸变的形成。

\begin{pics}[htbp]{激光雷达运动畸变}{distort}
    \addsubpic{ground truth}{width=0.3\textwidth}{distortion_groundtruth}
    \addsubpic{采集得到的数据}{width=0.3\textwidth}{distortion}
\end{pics}

图\ref{distortion_groundtruth}中的黑色的线条表示二维激光雷达处在的真实环境的轮廓图，箭头表示二维激光雷达的运动方向。图\ref{distortion}中的蓝色的线条表示二维激光雷达的原始数据。注意到其已经发生了畸变，因为二维激光雷达内的激光测距器通过逆时针的方向旋转，使得右上角的数据优先得到，而左上角的数据在激光雷达向箭头方向运动了一段距离之后才进行测量，自然导致了运动畸变的产生。

值得一提的是上图表示的二维激光雷达发生的运动畸变是当激光雷达运动方向为水平运动方向时造成的，而当激光雷达在空间中有垂直方向的旋转运动时，其造成的运动畸变远比水平运动严重。这是因为激光雷达旋转一周的时间普遍在0.1秒左右，其水平运动的距离往往很小可以忽略不计。而当激光雷达有竖直方向的旋转时，即使在0.1s内只有2度的航向角的旋转（这在本文的机构中并不算很快），在测量20m处的物体时，其运动造成的点云畸变可使得点云的同一线上的第一个点与最后一个点的垂直相差将近70cm。

综上所述，对于第二章所述的机构，由于其施加了在激光雷达航向角方向上的旋转，因此导致其在竖直方向上的运动畸变不可忽视，从而简单的叠加点云会导致在做激光雷达的物体检测时的失真。

\subsubsection{运动畸变的矫正}

对于本文所提到的三维感知机构在航向角方向上的旋转所产生的运动畸变的矫正，一个较为朴素的方法是，依次遍历激光雷达每帧点云中的每个点，计算其产生的时间戳$t$，对磁编码器的角度进行插值，计算出在时间戳$t$上的角度$\alpha$，然后将该点绕原点在航向角方向旋转$-\alpha$的角度。这个方法最为直观，然而激光雷达每帧点云高达数十万个点，如果对每个测量得到的点进行插值，则在一秒内要进行近百万次的插值与旋转操作，显然对于无人驾驶汽车上的移动处理器平台来说这是不现实的。

因此本文提出一个假设，设点云中第一个点产生的时间戳为$t_0$，最后一个点产生的时间戳为$t_k$，则将$t_0$到$t_k$之间的时间均匀分为$n$份，每份长为$\Delta t$。本文假设$t_0$至$t_0+\Delta t$、$t_0+\Delta t$至$t_0+2\Delta t$...$t_k-\Delta t$至$t_k$这些时间段，每个时间段内的激光雷达的测距点的产生的时间都是相同的，为其第一个点的产生时间。根据这个假设，每个时间段内的所有点都只要进行相同角度变换即可进行运动畸变的矫正。遵循该假设，则每帧点云只需要进行n次角度插值即可，极大地减小了运算量。同时虽然该假设认为同一时间段内的点云是同一时间产生的，每个时间段内的点仍有运动畸变的影响，然而在实验过程中发现，只要当$n$取一个不太小的值（$n\geq 50$），则该假设的所产生的时间段内的运动畸变产生的影响很小，可忽略不计。

在实际的程序实现中，由于激光雷达每帧点云是分段传输的，如图\ref{udp_packet}所示。
\pic[htbp]{点云的传输}{width=0.6\textwidth}{udp_packet}
RS-LiDAR-16激光雷达采用UDP协议向PC传输点云信息，而UDP协议相较于TCP协议，发送数据之前不需要双方建立链接，并且发送的数据没有校验，也没有丢包的检测，因此不适合一次性发送大量数据给PC。该激光雷达将一帧点云分为84个 UDP包(UDP Packet)，每个包中的点云都是激光雷达旋转$360 / 84 = 4.28$度后得到的16线的点云的集合。当激光雷达的驱动程序接收到84个UDP Packet之后，将这84 个Packet合并成一帧点云输出。

本文修改了RS-LiDAR-16的ROS 驱动程序，将每个UDP Packet不经过合并直接发布出去，同时在多帧融合的程序里，对每个Packet（而不是每帧）分别进行编码器角度的插值与点云的旋转，最后再将旋转后的84个Packet进行合并发布。

\subsection{矫正运动畸变后的多帧融合策略}

如上文所言，对单帧点云进行多次插值之后的多帧融合算法流程如Algorithm~\ref{fusion_algorithm}所示；。

\begin{algorithm}[t]
    \caption{Improved multiple pointcloud fusion algorithm} %算法的名字
    \label{fusion_algorithm}
    \begin{algorithmic}[1]
    % \State some description % \State 后写一般语句
    \For{packet $\in$ point cloud} % For 语句，需要和EndFor对应
        \State $\alpha_1$, $\alpha_2$ = FindeClosestAngles(encorderAngles, packet.timestamp)
    　　 \State $\alpha$ = $\alpha_1+(\alpha_2-\alpha_1)\times \frac{packet.timestamp - \alpha_1.timestamp}{\alpha_2.timestamp - \alpha_1.timestamp}$
        \State rectifiedPacket = RotatePointCloudByYaw(packet, $-\alpha$)
        \State rectifiedPointcloud += rectifiedPacket
    \EndFor
    \State \Return rectifiedPointcloud
    \end{algorithmic}
\end{algorithm}

在Algorithm~\ref{fusion_algorithm}中，FindeClosestAngles函数查找在所有的编码器角度中，时间上离packet的时间戳最近的两帧编码器的角度值，随后对这两个角度进行线性插值得到packet时间戳下的机构的角度。RotatePointCloudByYaw函数将特定的点云绕激光雷达坐标系原点旋转指定的角度。最后将每个矫正后的packet合并到一个新的矫正后的点云中并发布出去即得到了矫正因机构而产生的运动畸变后的点云。

在图\ref{car_after}中展示了消除运动畸变后的激光雷达测得的轿车的点云图案。相较于图\ref{car_before}，其点云图案没有出现明显的失真，并且通过机构进行多帧融合后的点云，能够明显的看出轿车的轮廓细节信息，包括车的反光镜、前挡风玻璃等。而矫正畸变前的轿车点云则很难分辨出这些细节，并且由于运动畸变的作用，其体积明显比矫正后的点云更大一些。由此证明了本文提出的多帧融合算法拥有较好的矫正畸变的效果。

\begin{pics}[htbp]{多帧融合后的轿车点云}{distort_remove}
    \addsubpic{矫正畸变前}{width=0.3\textwidth}{car_before}
    \addsubpic{矫正畸变后}{width=0.329\textwidth}{car_after}
\end{pics}

\section{激光雷达与相机的标定}

自动驾驶无人车是一个多传感器的系统，多传感器信息的融合可以使整个无人车系统的决策更加智能。激光雷达虽然能够获取较为精确的点云信息，然而点云信息只包含了三维距离信息。而相机可以通过图像获得大量信息诸如颜色、纹理信息等，但是其受光照与天气条件影响严重，并且从单目图像中无法获取三维结构信息。为了同时收集三维信息与物体的颜色与纹理信息，激光雷达与相机经常进行传感器的数据融合，来为多传感器系统提供更稳定的数据支持。为了进行数据的融合，首先得知道相机坐标系与激光雷达坐标系之间的旋转与平移关系，因此，激光雷达与相机的标定就显得尤为重要了。

本章节后续将介绍一种文献\citeup{dhall2017lidar}提到的,利用两张贴有ArUco Marker\citeup{garrido2014automatic}的标定板所提供的3d-3d特征匹配的方法，来进行相机与激光雷达的标定。

\subsection{标定板}
本文参照文献\citeup{dhall2017lidar}提到的方法，制作了两块长约40cm，宽约27cm的长方形硬纸板材质的标定板，并且将两块ArUco Marker粘在硬纸板的固定位置上，如图\ref{calib_board}所示。虽然一块标定板已经可以得到四组3d-3d匹配点来解决标定问题，本文仍然采用了两块标定板，目的是构造多于四对的匹配点来减小标定误差。

\pic[htbp]{标定环境}{width=0.8\textwidth}{calib_board}

\subsection{相机坐标系中的三维特征点提取}

ArUco markers是一种经过特定编码的二维码图案，用以实现对二维码自身的定位与畸变矫正。更多细节可以参考文献\citeup{garrido2014automatic}。该文献提出，通过特定的机器视觉算法检测到marker的四个角点后，可以对marker上的二维码进行解码运算，进而求得二维码的id与四个角点的顺序。而通过输入marker的边长后，还能够通过PnP\citeup{Lepetit2008}求解出相机坐标系到marker自身坐标系的转换。

\pic[htbp]{ArUco markers}{width=0.3\textwidth}{aruco_markers}

在本文中，将ArUco marker粘在硬纸板的矩形标定板上，并且测量得出硬纸板的四条边长以及marker在硬纸板中的位置，即可得到硬纸板的四个角点在marker 坐标系中的位置。而本文通过ROS中aruco\_ros以及aruco\_mapping\citeup{Bacik2016aruco}两个程序包可以检测ArUco marker的位置，进而得到相机坐标系到marker坐标系的转换，从而得到相机坐标系下的硬纸板的四个角点的位置。相机坐标系下角点位置的计算公式为

$$\begin{pmatrix} x_{camera}\\y_{camera}\\z_{camera}\\ 1 \end{pmatrix}\quad = \begin{pmatrix} R_{11}&R_{12}&R_{13}&t_x\\R_{21}&R_{22}&R_{23}&t_y\\R_{31}&R_{32}&R_{33}&t_z\\ 0&0&0&1 \end{pmatrix}\quad \begin{pmatrix} x_{aruco}\\y_{aruco}\\z_{aruco}\\ 1 \end{pmatrix}\quad$$
    
其中，$x_camera$是相机坐标系中角点的坐标，$x_aruco$是ArUco marker坐标系中的角点坐标，而上式中的$[R|t]$矩阵则是相机坐标系相对于marker坐标系的欧式变换矩阵。

\subsection{LiDAR坐标系中的三维特征点提取}

本文所参照的标定方法，其在LiDAR坐标系中的三维特征点提取是通过直线拟合的方法进行的。如图\ref{line_fitting}所示。图中显示的点为激光雷达的点云投影到相机图像上之后进行边缘检测后所形成的点。在得到该幅图像后，需要手动框选标定板的每条边上的点，随后标定程序会对这些点进行直线拟合，每两条直线的交点即为所求的LiDAR坐标系中的三维点。

\pic[htbp]{直线拟合提取特征点}{width=0.8\textwidth}{line_fitting}
\subsection{刚体变换求解}

在得到两个坐标系下的三维特征匹配点之后，两个坐标系之间的$[R|t]$刚体变换矩阵可以通过使用迭代最近点（Iterative Closest Point, ICP）\citeup{Zhang1994Iterative}算法求得。假设$P$，$Q$为$\mathbb{R}^3$中的一组对应点，ICP算法尝试使经过刚体变换后的P点集与Q点集的重合误差最小。其求解问题可以表述为式\ref{eq:ICP}所示。
\begin{equation}
    \mathop{\arg\min}_{R\in SO(3), t\in \mathbb{R}^3}  \| (RP+t)-Q \|^2 
    \label{eq:ICP}
\end{equation}
一般来说，ICP问题认为对于点集$P$中的每个点，在点集$Q$中与之对应的点为其距离最近的点，依次确认匹配关系后，该算法通过减小两个点集的欧氏距离来对齐两个点集。

ICP的方法找到两组点的匹配关系，在没有良好的初始值的情况下，极易发生误匹配，导致算法最后没办法收敛到一个正确的解。考虑到在本文的标定环境下，两组点的对应关系的是已知的，则两组点之间的$[R|t]$即存在一个闭式解。Kabsch算法\citeup{kabsch1976solution}\citeup{sorkine2009least}提供了找到两组对应点间的旋转矩阵的闭式解的方法，而两组点间的平移量可以在进行旋转对齐后求得。下面介绍Kabsch算法的主要步骤。

首先假设旋转已知，求两个点集$P$与$Q$之间的平移量，设
\begin{equation}
    F(t) = \sum_{i=1}^n{\|(RP_i + t)-Q_i \|}^2
    \label{eq:k1}
\end{equation}
对上式进行求导，令两边等于0，得
\begin{equation}
    \frac{\partial F(t)}{\partial t}=2\sum_{i=1}^n{((RP_i + t)-Q_i)}=0
\end{equation}
因为
\begin{equation}
    \frac{\partial F(t)}{\partial t}=2R\sum_{i=1}^n{P_i} +2nt - 2\sum_{i=1}^n{Q_i}
\end{equation}
可得
\begin{equation}
    t = \frac{1}{n}\sum_{i=1}^n{Q_i}-R\frac{1}{n}\sum_{i=1}^n{P_i}
\end{equation}
\begin{equation}
    t = \overline{Q}-R\overline{P}
    \label{eq:k2}
\end{equation}
将式\ref{eq:k2}的结果代入式\ref{eq:k1}中，有
\begin{equation}
    R = \mathop{\arg\min}_{R \in SO(3)} \sum_{i=1}^n\|(R(P_i - \overline{P})-(Q_i - \overline{Q}))\|^2
\end{equation}
令
$$
    X = P_i - \overline{P}, X' = RX, Y= Q_i - \overline{Q}
$$
则目标函数变为
\section{本章小结}

